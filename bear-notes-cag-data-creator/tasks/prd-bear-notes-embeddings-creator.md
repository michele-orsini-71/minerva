# Product Requirements Document: Bear Notes Embeddings Creator

## Introduction/Overview

The Bear Notes Embeddings Creator is a command-line tool that processes exported Bear notes data and creates vector embeddings for use in a Retrieval-Augmented Generation (RAG) system. The tool takes JSON output from the bear-notes-parser and creates a searchable vector database using ChromaDB, enabling semantic search and AI-powered question answering over personal note collections.

This tool addresses the problem of finding relevant information across large collections of personal notes by creating semantic embeddings that can understand context and meaning beyond simple keyword matching.

## Goals

1. **Process Bear notes efficiently**: Convert JSON-formatted Bear notes into vector embeddings suitable for semantic search
2. **Create robust chunking**: Intelligently segment long notes while preserving semantic boundaries and code blocks
3. **Generate high-quality embeddings**: Use local AI models to create vector representations that capture semantic meaning
4. **Populate ChromaDB reliably**: Store embeddings with comprehensive metadata for retrieval and provenance
5. **Provide clear feedback**: Give users visibility into processing progress and results

## User Stories

**As a Bear notes user**, I want to process my exported notes into a vector database so that I can later perform semantic searches across my knowledge base.

**As a Bear notes user**, I want the tool to handle processing errors gracefully so that a few corrupted notes don't prevent me from creating embeddings for the rest of my collection.

**As a Bear notes user**, I want to see processing progress so that I know the tool is working and can estimate completion time.

**As a Bear notes user**, I want a summary report after processing so that I can verify all my notes were processed successfully and understand what was created.

## Functional Requirements

### Core Processing Requirements

1. **JSON Input Processing**: The system must accept JSON files generated by the bear-notes-parser containing note data with fields: title, markdown, size, modificationDate, creationDate.

2. **Intelligent Chunking**: The system must split note content into chunks using LangChain text splitters while:
   - Preserving code blocks and tables as atomic units (no splitting) using RecursiveCharacterTextSplitter
   - Respecting semantic boundaries (headings, paragraphs, sentences) using MarkdownHeaderTextSplitter
   - Using character-based sizing with configurable chunk_size and chunk_overlap parameters
   - Using optimized chunk size determined through testing methodology (default: 1200 characters target, 200 characters overlap)
   - Implementing configurable overlap between consecutive chunks using LangChain's built-in overlap mechanism

3. **Vector Embedding Generation**: The system must generate embeddings using:
   - Local Ollama service with mxbai-embed-large:latest model
   - L2 normalization for cosine similarity compatibility
   - Batch processing for efficiency

4. **ChromaDB Storage**: The system must store embeddings with:
   - HNSW index configuration with cosine distance metric
   - Persistent client (folder-based storage)
   - **Default storage path**: `../chromadb_data/bear_notes_embeddings`
   - **Collection name**: `bear_notes_chunks`
   - **Processing mode**: Full collection rebuild (delete existing collection before processing)
   - **CLI override**: `--chromadb-path` option for custom storage location
   - Stable chunk IDs using SHA256(note_id | modificationDate | chunk_index)
   - Complete metadata including note_id, title, modificationDate, creationDate, size, chunk_index
   - Note: note_id derived from SHA1(title + creationDate) to ensure uniqueness

5. **Error Resilience**: The system must continue processing when individual notes fail, collecting and reporting errors at completion.

### User Interface Requirements

6. **Command Line Interface**: The system must provide a CLI that accepts:
   - JSON file path as input argument (required)
   - `--chunk-size` option (default: 1200 characters) - maps to LangChain RecursiveCharacterTextSplitter.chunk_size
   - `--overlap-chars` option (default: 200 characters) - maps to LangChain RecursiveCharacterTextSplitter.chunk_overlap
   - `--chromadb-path` option (default: `../chromadb_data/bear_notes_embeddings`) [to be implemented]
   - `--verbose` flag for detailed logging
   - `--output` option for saving enriched notes with chunks
   - Help and usage information

7. **Progress Reporting**: The system must display:
   - Current progress percentage (by notes processed: "Processing note 15/100 (15%)")
   - Name/title of note currently being processed
   - Real-time status updates during long operations
   - Update frequency: after each note completion
   - Output format: stdout text with optional `--json` flag for structured output

8. **Summary Reporting**: The system must generate completion reports showing:
   - Total notes processed successfully
   - Total chunks created
   - Number and details of any failed notes
   - Processing time and performance metrics
   - Output format: stdout text summary, with optional `--json` flag for structured JSON output
   - Report persistence: stdout only (no file output unless explicitly requested)

### Testing Requirements

9. **Comprehensive Test Coverage**: The system must include automated tests for each pipeline phase:

   **JSON Input Processing Tests:**
   - Validate parsing of well-formed Bear notes JSON files
   - Handle malformed JSON gracefully with clear error messages
   - Test with various note sizes and character encodings (UTF-8, special characters)
   - Verify correct extraction of all required fields (title, markdown, size, modificationDate, creationDate)

   **LangChain Chunking Algorithm Tests (CRITICAL - Comprehensive Coverage Required):**
   - **Code Block Preservation**: Test that LangChain RecursiveCharacterTextSplitter preserves fenced code blocks (```), indented code blocks, and inline code spans
   - **Table Integrity**: Verify LangChain splitters keep markdown tables complete within single chunks
   - **Heading Boundary Respect**: Test that MarkdownHeaderTextSplitter breaks at heading boundaries and preserves heading metadata
   - **Paragraph Preservation**: Ensure LangChain splitters don't split paragraphs mid-sentence using proper separators
   - **Character-based Sizing**: Test LangChain chunk_size parameter accuracy with character-based length function
   - **Overlap Calculation**: Verify LangChain chunk_overlap parameter works correctly at character boundaries
   - **Complex Markdown Structures**: Test nested lists, blockquotes, mixed content (text + code + lists)
   - **Edge Cases**:
     - Very short notes (< minimum chunk size)
     - Very long notes (> maximum reasonable chunk count)
     - Notes with only code blocks
     - Notes with extremely long code blocks (> target chunk size)
     - Notes with complex nested heading structures (H1 > H2 > H3 > H4)
     - Notes with mixed language content and special Unicode characters
     - Empty notes or notes with only whitespace
   - **Boundary Detection**: Test proper sentence and paragraph boundary detection
   - **Performance**: Test chunking performance with very large notes (>100KB)
   - **Consistency**: Verify identical input produces identical chunking results

   **Embedding Generation Tests:**
   - Test consistency of embeddings for identical input text
   - Verify L2 normalization is applied correctly
   - Test batch processing efficiency and correctness
   - Mock Ollama service for offline testing
   - Test error handling when Ollama service is unavailable

   **ChromaDB Storage Tests:**
   - Test stable chunk ID generation and uniqueness
   - Test note_id uniqueness with duplicate note titles (same title, different creation dates)
   - Verify metadata storage and retrieval accuracy
   - Test batch insertion performance and correctness
   - Test collection creation and configuration (HNSW, cosine distance)
   - Test persistence across application restarts

   **Integration Tests:**
   - End-to-end processing of complete note collections
   - Test progress reporting accuracy throughout pipeline
   - Test error handling and recovery for each phase
   - Performance benchmarks with various collection sizes

### Data Management Requirements

10. **Metadata Preservation**: The system must store comprehensive metadata for each chunk:
   - note_id (stable identifier derived from note title + creationDate to ensure uniqueness)
   - Original note title from Bear
   - modificationDate in UTC ISO format
   - creationDate in UTC ISO format
   - Original note size in bytes
   - chunk_index within the note
   - Optional: char_start and char_end positions for provenance

11. **Processing Strategy**: The system implements full collection rebuild for MVP (deletes existing collection before processing). Stable chunk IDs enable future incremental updates as a post-MVP enhancement.

12. **Chunk Size Optimization**: The system must implement a methodology to determine optimal chunk size:
   - **When to use**: During development phase only, not during regular tool execution
   - **Purpose**: One-time research to establish optimal default chunk size for the tool
   - **Automatic Dataset Selection**: Implement automatic selection of 10-12 evaluation notes from complete Bear notes JSON:

     **Selection Algorithm Requirements:**
     - Analyze all notes in input JSON to automatically identify representatives for each category
     - Generate selection report showing which notes were chosen and scoring rationale
     - Ensure no note overlap (each note serves only one category)
     - Provide reproducible selection based on content analysis

     **Length Variety (6 notes) - Automatic Selection:**
     - 2 short notes (< 500 characters) - select by character count analysis
     - 2 average notes (500-2000 characters) - select from middle range
     - 2 long notes (> 2000 characters) - select longest available

     **Content Structure Variety (6 notes) - Pattern Matching:**
     - 1 note with markdown tables (scan for `|` table patterns)
     - 1 code-heavy note (detect ``` fenced blocks and indented code)
     - 1 heading-rich note (count # heading levels, prefer deep hierarchy)
     - 1 list-heavy note (detect complex nested `*`/`-` list patterns)
     - 1 mixed content note (score combination of text + lists + code + formatting)
     - 1 plain text note (minimal markdown, mostly prose content)

     **Edge Cases/Special Features (2-3 notes) - Content Analysis:**
     - 1 very short note (< 100 characters, minimum available)
     - 1 Unicode-heavy note (detect special characters, emojis, non-Latin scripts)
     - 1 blockquotes/links note (scan for `>` blockquotes and `[]()`/`<>` link patterns)

   - Test multiple chunk sizes: 800, 1200, 1600, and 2000 characters (~200, 300, 400, and 500 token equivalents at ~4 chars/token)
   - Evaluate each chunk size against measurable criteria:
     - **Semantic coherence**: Manual review - chunks contain complete thoughts/concepts
     - **Boundary preservation**: Automated validation - code blocks and tables remain intact
     - **Storage efficiency**: Metric - reasonable number of chunks per note (target: 3-10 chunks for average notes)
     - **Embedding quality**: Simplified approach - start with manual assessment of chunk quality, detailed retrieval testing can be added later
   - Document selected chunk size with rationale for future re-evaluation
   - Use selected optimal size as default for production processing
   - Note: Character-based approach required due to mxbai-embed-large model not exposing tokenizer

## Non-Goals (Out of Scope)

1. **Query Interface**: This tool does not provide search or query capabilities - it only creates the vector database.

2. **Bear App Integration**: No direct integration with the Bear application itself - works only with exported JSON data.

3. **Real-time Processing**: No live synchronization with Bear notes - designed for periodic manual runs.

4. **Multi-user Support**: Designed for single-user personal use only.

5. **Web Interface**: Command-line tool only - no web UI or API endpoints.

6. **Backup Management**: Does not manage ChromaDB backups or versioning.

7. **Advanced Analytics**: No analysis of note content patterns, usage statistics, or optimization recommendations.

## Technical Considerations

### Dependencies

- Python 3.13+ (matches existing project environment)
- ChromaDB for vector storage
- Ollama service and mxbai-embed-large model for embeddings
- LangChain text splitters (langchain-text-splitters) for intelligent chunking
- Existing bear-notes-parser JSON output format compatibility

### Architecture Constraints

- Must work completely offline (no internet required)
- Should integrate with existing project structure and virtual environment
- Must be compatible with ChromaDB persistent storage in ../chromadb_data/

### Performance Considerations

- Batch processing for ChromaDB operations with optimal batch size determined through testing (test sizes: 32, 64, 96, 128 chunks per batch)
- Memory-efficient processing for large note collections
- Progress reporting without significant performance impact

### Error Handling Strategy

- Continue processing on individual note failures
- Comprehensive error logging and reporting
- Graceful degradation when optional features fail

## Design Considerations

### Code Organization

- Follow existing project patterns from test-chunking.py reference implementation
- Modular design separating chunking, embedding, and storage concerns
- Clear separation between CLI interface and core processing logic

### Configuration Management

- Command-line arguments for common parameters
- Sensible defaults based on research (1200-2000 characters â‰ˆ 300-500 tokens, 10-20% overlap)
- Future extensibility for configuration files

### Logging and Debugging

- Detailed logging for troubleshooting
- Optional verbose mode for development and debugging
- Clear error messages for common failure scenarios

## Success Metrics

1. **Processing Accuracy**: Successfully processes 95%+ of valid Bear notes without errors
2. **Data Integrity**: All successfully processed notes can be retrieved with correct metadata
3. **Performance**: Processes typical note collections (hundreds of notes) within reasonable time (under 30 minutes)
4. **Usability**: Clear progress feedback and error reporting enable self-service troubleshooting

## Testing Environment Setup

**ChromaDB Testing Strategy**: Automated tests will use a real ChromaDB instance with a dedicated test database name (e.g., "bear_notes_test") to ensure full integration testing. While this approach requires more time than mocking, it provides:

- Complete end-to-end validation of ChromaDB operations
- Real-world testing of HNSW index configuration and cosine distance
- Verification of actual persistence and retrieval behavior
- Detection of ChromaDB-specific issues that mocks might miss

**Test Database Management**:
- Use separate test database name to avoid conflicts with production data
- Clean up test database before/after test runs
- Ensure test isolation between different test cases

## Future Enhancements

### Advanced Metadata-Enhanced RAG Features

The current implementation stores header metadata from LangChain's MarkdownHeaderTextSplitter in the `header_metadata` field of each chunk. This foundation enables several advanced RAG capabilities that can be implemented in future iterations:

#### 1. **Metadata-Based Filtering**
- **Pre-filtering**: Filter chunks before vector search using ChromaDB's `where` clause
- **Post-filtering**: Refine vector search results using metadata criteria
- **Dynamic filtering**: Use LLM-based query analysis to automatically extract metadata filters

**Example Implementation:**
```python
# User query: "Show me Vizzani project logs from December 2022"
# Extracted filters:
filters = {
    "header_metadata.Header 1": {"$eq": "201909 Vendita Vizzani"},
    "header_metadata.Header 2": {"$eq": "Log"},
    "header_metadata.Header 3": {"$gte": "20221201", "$lt": "20230101"}
}
results = collection.query(query_texts=["user query"], where=filters)
```

#### 2. **Context-Aware Generation Enhancement**
- Include header path context in prompts for improved AI understanding
- Preserve document structure information for better response coherence
- Enable hierarchical context reconstruction for multi-section queries

#### 3. **Advanced Query Processing**
- **LLM-based query classification**: Automatically extract project names, dates, content types
- **Keyword/pattern matching**: Use regex to identify filterable terms in user queries
- **Intent routing**: Direct queries to appropriate document sections based on metadata

#### 4. **Hybrid Search and Re-ranking**
- Combine vector similarity with metadata relevance scoring
- Re-rank results based on structural context and metadata matches
- Multi-modal retrieval using both semantic content and document structure

#### 5. **ChromaDB Schema Enhancement**
When implementing ChromaDB storage, ensure the schema supports metadata fields:
```python
chunk_metadata = {
    'note_id': note_id,
    'title': note['title'],
    'modificationDate': note['modificationDate'],
    'chunk_index': chunk_index,
    'header_metadata': chunk_data['metadata']  # Store header path
}
```

### Implementation Priority
These enhancements should be considered after MVP completion, with metadata filtering being the highest-impact improvement for retrieval quality.

## Implementation Decisions

**Deferred Design Decisions** (reasonable defaults, can be refined during implementation):

1. **Ollama Service Discovery**: Standard localhost:11434 endpoint, clear error messages if service unavailable or model missing
2. **ChromaDB Batch Size**: Default 64 chunks per batch operation - good balance of performance and memory usage
3. **Exit Code Strategy**: Return non-zero exit code if any notes fail processing - standard Unix convention for partial failures
4. **Test Fixtures**: Use real user notes from Bear collection for comprehensive testing - synthetic fixtures acceptable for edge cases
5. **Performance Benchmarking**: Log batch operation timings, select fastest performing batch size during development testing - no runtime auto-tuning initially

