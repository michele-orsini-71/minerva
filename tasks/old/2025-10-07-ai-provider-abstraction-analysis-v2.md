# AI Provider Abstraction Layer Analysis (v2)

## Switching Between Ollama and OpenAI-compatible Services

**Version:** 2.0
**Date:** 2025-10-07
**Status:** Implementation-Ready

---

## Important Design Constraint

âš ï¸ **CRITICAL:** The embedding of notes data (CAG data creator) and the embedding of the user question (MCP server) **MUST** be done with the **SAME** engine.

**Implementation requirement:** The embedding engine specification must:

1. Appear in the ChromaDB collection metadata at creation time
2. Be validated by the MCP server at query time to verify embedding engine compatibility

---

## Executive Summary

**TL;DR: YES, it's highly feasible to add AI provider abstraction to support both Ollama (local) and OpenAI/Gemini/etc. (remote) services.** The codebase is well-structured for this change, requiring minimal refactoring. Using LiteLLM as an abstraction layer would provide unified access to 100+ AI providers with a single API.

---

## Implementation Decisions (FINALIZED)

The following architectural decisions have been finalized:

### 1. Configuration File Strategy âœ“

- **Two separate JSON config files** (one per tool)
  - `markdown-notes-cag-data-creator/config.json` - Pipeline configuration
  - `markdown-notes-mcp-server/config.json` - MCP server configuration
- **API keys via environment variables only** - No secrets stored in config files
- **Config files CAN be committed to git** - They contain template references like `"${OPENAI_API_KEY}"`

### 2. Embedding Dimension Validation âœ“

- **Hard error on mismatch** - No warnings, complete blocking
- Validation occurs at query time in MCP server
- Clear error message explaining the mismatch

### 3. Provider Initialization âœ“

- **Assertion-based protection** for uninitialized provider
- Programming error (not user error) - should never happen in production
- Clear error messages for debugging

### 4. Backward Compatibility âœ“

- **REMOVED** - No existing users, clean slate implementation
- No legacy compatibility shims
- No deprecation warnings needed

### 5. API Key Security âœ“

- **Environment variable injection only**
- Config files reference env vars: `"${OPENAI_API_KEY}"`
- Resolution happens at runtime via `AIProviderConfig._resolve_api_key()`
- No `.env` files, no secrets in config files

### 6. Metadata Flow âœ“

- **Config â†’ Provider â†’ ChromaDB collection metadata**
- No hardcoded model values anywhere
- Metadata flows through the entire pipeline
- Specific implementation in `AIProvider.get_embedding_metadata()`

### 7. LiteLLM Model Naming âœ“

- **Model names come directly from config**
- LiteLLM auto-detects provider from model string format
- No prefix transformations needed
- `provider_type` is for metadata only

### 8. Testing Strategy âœ“

- **Post-implementation testing**
- Priority: Ollama first, cloud providers optional
- No CI/CD constraints for API key exposure

---

## Current AI Usage Points

I've identified **THREE distinct AI operations** across the pipeline and MCP server:

### 1. **Vector Embedding Creation** (Primary Use)

**Current Implementation:**

- **Location:** `markdown-notes-cag-data-creator/embedding.py`
- **Model:** `mxbai-embed-large:latest` via Ollama
- **Usage:** Converting text chunks to 1024-dimensional vectors
- **Frequency:** Batch processing during pipeline runs (hundreds to thousands of calls)

**Code Pattern:**

```python
from ollama import embeddings as ollama_embeddings

def generate_embedding(text: str, model: str = EMBED_MODEL):
    response = ollama_embeddings(model=model, prompt=text)
    vector = np.array(response['embedding'])
    normalized = l2_normalize(vector.reshape(1, -1))
    return normalized.flatten().tolist()
```

### 2. **Collection Description Validation** (AI-Enhanced QA)

**Future Implementation (PRD Spec):**

- **Location:** Will be added to `markdown-notes-cag-data-creator/full_pipeline.py`
- **Model:** Configured LLM (e.g., `llama3.1:8b`)
- **Usage:** Validating collection descriptions for AI-friendliness (scoring 0-10)
- **Frequency:** Once per collection creation (low volume)

**Expected Code Pattern (from PRD):**

```python
# TIER 2: Optional AI validation
ai_result = validate_with_ai(description)  # Uses configured LLM
if not ai_result["passed"]:
    issues.append(f"AI validation failed (score: {ai_result['score']}/10)")
```

### 3. **Query Embedding Generation** (Search-time)

**Current Implementation:**

- **Location:** `test-files/chromadb_query_client.py` (will move to MCP server)
- **Model:** Same embedding model as pipeline
- **Usage:** Converting user queries to embeddings for semantic search
- **Frequency:** Per user query (interactive, low-to-medium volume)

**Code Pattern:**

```python
def generate_query_embedding(query: str):
    embedding = generate_embedding(query.strip())  # Reuses embedding.py
    return embedding
```

---

## Proposed Abstraction Architecture

### Solution: LiteLLM Abstraction Layer

**LiteLLM** is the ideal choice for this use case:

- âœ… Unified API for 100+ providers (OpenAI, Anthropic, Google Gemini, Azure, Ollama, etc.)
- âœ… **Supports both embeddings AND completions** (covers all 3 use cases)
- âœ… Drop-in replacement for OpenAI SDK
- âœ… Automatic fallback/retry logic
- âœ… Standardized response format across providers
- âœ… Local caching support

### Configuration-Driven Provider Selection

#### Two-Config Architecture

```
Project Structure:
â”œâ”€â”€ markdown-notes-cag-data-creator/
â”‚   â”œâ”€â”€ config.json              â† Pipeline config
â”‚   â”œâ”€â”€ config.example.json      â† Template (committed to git)
â”‚   â””â”€â”€ ...
â””â”€â”€ markdown-notes-mcp-server/
    â”œâ”€â”€ config.json              â† MCP server config
    â”œâ”€â”€ config.example.json      â† Template (committed to git)
    â””â”€â”€ ...
```

**Configuration Schema (Ollama Example):**

```json
{
  "chromadb_path": "../chromadb_data",
  "default_max_results": 5,

  "ai_provider": {
    "provider_type": "ollama",

    "embedding_model": {
      "model": "mxbai-embed-large:latest",
      "base_url": "http://localhost:11434",
      "api_key": null
    },

    "llm_model": {
      "model": "llama3.1:8b",
      "base_url": "http://localhost:11434",
      "api_key": null
    }
  }
}
```

**OpenAI Example (with environment variable reference):**

```json
{
  "chromadb_path": "../chromadb_data",
  "default_max_results": 5,

  "ai_provider": {
    "provider_type": "openai",

    "embedding_model": {
      "model": "text-embedding-3-large",
      "api_key": "${OPENAI_API_KEY}"
    },

    "llm_model": {
      "model": "gpt-4o-mini",
      "api_key": "${OPENAI_API_KEY}"
    }
  }
}
```

**Gemini Example:**

```json
{
  "chromadb_path": "../chromadb_data",
  "default_max_results": 5,

  "ai_provider": {
    "provider_type": "gemini",

    "embedding_model": {
      "model": "text-embedding-004",
      "api_key": "${GEMINI_API_KEY}"
    },

    "llm_model": {
      "model": "gemini-1.5-flash",
      "api_key": "${GEMINI_API_KEY}"
    }
  }
}
```

---

## Implementation Plan

### Phase 1: Create AI Provider Abstraction Layer

**New Module:** `markdown-notes-cag-data-creator/ai_provider.py`

```python
#!/usr/bin/env python3
"""
AI Provider Abstraction Layer
Supports multiple embedding and LLM providers via LiteLLM
"""

import os
import json
from typing import List, Dict, Any, Optional, Literal
from litellm import embedding, completion
import numpy as np


ProviderType = Literal["ollama", "openai", "gemini", "azure", "anthropic"]


class AIProviderConfig:
    """Configuration for AI provider"""
    def __init__(
        self,
        provider_type: ProviderType,
        embedding_model: str,
        llm_model: str,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        self.provider_type = provider_type
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.base_url = base_url
        self.api_key = self._resolve_api_key(api_key)

    def _resolve_api_key(self, api_key: Optional[str]) -> Optional[str]:
        """
        Resolve API key from environment variables if needed.

        Supports syntax: "${ENV_VAR_NAME}" which gets resolved to os.environ["ENV_VAR_NAME"]
        """
        if api_key and api_key.startswith("${") and api_key.endswith("}"):
            env_var = api_key[2:-1]  # Extract "OPENAI_API_KEY" from "${OPENAI_API_KEY}"
            value = os.environ.get(env_var)
            if value is None:
                raise ValueError(
                    f"Environment variable '{env_var}' not set. "
                    f"Please export {env_var}=<your-api-key>"
                )
            return value
        return api_key


class AIProvider:
    """Unified interface for AI operations across providers"""

    def __init__(self, config: AIProviderConfig):
        self.config = config

    def generate_embedding(
        self,
        text: str,
        normalize: bool = True
    ) -> List[float]:
        """
        Generate embedding using configured provider.

        Args:
            text: Input text to embed
            normalize: Apply L2 normalization (for cosine similarity)

        Returns:
            Embedding vector as list of floats
        """
        try:
            response = embedding(
                model=self.config.embedding_model,
                input=[text],
                api_key=self.config.api_key,
                api_base=self.config.base_url
            )

            vector = np.array(response.data[0]['embedding'], dtype=np.float32)

            if normalize:
                vector = self._l2_normalize(vector)

            return vector.tolist()

        except Exception as e:
            raise AIProviderError(f"Embedding generation failed: {e}")

    def generate_embeddings_batch(
        self,
        texts: List[str],
        normalize: bool = True,
        progress_callback: Optional[callable] = None
    ) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        embeddings = []

        for i, text in enumerate(texts):
            if progress_callback:
                progress_callback(i, len(texts))

            emb = self.generate_embedding(text, normalize=normalize)
            embeddings.append(emb)

        if progress_callback:
            progress_callback(len(texts), len(texts))

        return embeddings

    def validate_description(
        self,
        description: str,
        threshold: int = 7
    ) -> Dict[str, Any]:
        """
        Use LLM to validate collection description quality.

        Args:
            description: Collection description to validate
            threshold: Minimum score to pass (0-10)

        Returns:
            {"passed": bool, "score": int, "reason": str}
        """
        prompt = f"""
        Rate this collection description on a scale of 0-10 for AI agent usability.
        A good description clearly explains:
        1. What content is in the collection
        2. When an AI should query this collection
        3. What types of questions it answers

        Description to rate:
        "{description}"

        Respond in JSON format:
        {{
            "score": <0-10>,
            "reason": "<brief explanation>"
        }}
        """

        try:
            response = completion(
                model=self.config.llm_model,
                messages=[{"role": "user", "content": prompt}],
                api_key=self.config.api_key,
                api_base=self.config.base_url,
                response_format={"provider_type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            score = result.get("score", 0)

            return {
                "passed": score >= threshold,
                "score": score,
                "reason": result.get("reason", "No reason provided")
            }

        except Exception as e:
            raise AIProviderError(f"Description validation failed: {e}")

    def get_embedding_metadata(self) -> Dict[str, Any]:
        """
        Get embedding model metadata for storage in ChromaDB.

        This metadata will be used to validate query-time compatibility
        and ensure the same embedding model is used for both indexing
        and querying.

        Returns:
            Dictionary with embedding configuration metadata
        """
        # Generate test embedding to determine actual dimension
        test_emb = self.generate_embedding("test", normalize=False)

        return {
            "embedding_model": self.config.embedding_model,
            "embedding_dimension": len(test_emb),
            "embedding_provider": self.config.provider_type,
            "embedding_base_url": self.config.base_url
        }

    def _l2_normalize(self, vector: np.ndarray) -> np.ndarray:
        """Apply L2 normalization for cosine similarity"""
        norm = np.linalg.norm(vector)
        if norm == 0:
            return vector
        return vector / norm

    def check_availability(self) -> Dict[str, Any]:
        """Check if provider is available and responsive"""
        try:
            # Test with a simple embedding
            test_embedding = self.generate_embedding("test", normalize=False)

            return {
                "available": True,
                "provider": self.config.provider_type,
                "embedding_model": self.config.embedding_model,
                "llm_model": self.config.llm_model,
                "embedding_dim": len(test_embedding)
            }
        except Exception as e:
            return {
                "available": False,
                "error": str(e)
            }


class AIProviderError(Exception):
    """Exception raised when AI provider operations fail"""
    pass


def load_provider_from_config(config_path: str) -> AIProvider:
    """Load AI provider from configuration file"""
    from pathlib import Path

    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    with open(config_file) as f:
        config = json.load(f)

    ai_config = config.get("ai_provider", {})

    provider_config = AIProviderConfig(
        provider_type=ai_config.get("type", "ollama"),
        embedding_model=ai_config["embedding"]["model"],
        llm_model=ai_config["llm"]["model"],
        base_url=ai_config["embedding"].get("base_url"),
        api_key=ai_config["embedding"].get("api_key")
    )

    return AIProvider(provider_config)
```

---

### Phase 2: Refactor Existing Code

#### 2.1 Update `embedding.py`

**Before:**

```python
from ollama import embeddings as ollama_embeddings

def generate_embedding(text: str, model: str = EMBED_MODEL):
    response = ollama_embeddings(model=model, prompt=text)
    # ... processing
```

**After:**

```python
from typing import List, Optional
from ai_provider import AIProvider, load_provider_from_config, AIProviderError

# Global provider instance (loaded from config)
_provider: Optional[AIProvider] = None


def initialize_provider(config_path: str = "config.json") -> Dict[str, Any]:
    """
    Initialize AI provider from configuration.

    Args:
        config_path: Path to configuration file

    Returns:
        Provider status dictionary

    Raises:
        AIProviderError: If provider initialization or availability check fails
    """
    global _provider
    _provider = load_provider_from_config(config_path)
    return _provider.check_availability()


def generate_embedding(text: str) -> List[float]:
    """
    Generate embedding using configured AI provider.

    Args:
        text: Input text to embed

    Returns:
        Embedding vector as list of floats

    Raises:
        AssertionError: If provider not initialized (programming error)
    """
    assert _provider is not None, (
        "AI Provider not initialized! This is a programming error. "
        "Call initialize_provider(config_path) before using embedding functions."
    )

    return _provider.generate_embedding(text)


def generate_embeddings_batch(
    texts: List[str],
    progress_callback=None
) -> List[List[float]]:
    """
    Generate batch embeddings using configured AI provider.

    Args:
        texts: List of texts to embed
        progress_callback: Optional callback function(current, total)

    Returns:
        List of embedding vectors

    Raises:
        AssertionError: If provider not initialized (programming error)
    """
    assert _provider is not None, (
        "AI Provider not initialized! This is a programming error. "
        "Call initialize_provider(config_path) before using embedding functions."
    )

    return _provider.generate_embeddings_batch(texts, progress_callback=progress_callback)


def get_embedding_metadata() -> Dict[str, Any]:
    """
    Get embedding model metadata for ChromaDB collection.

    Returns:
        Metadata dictionary with embedding configuration

    Raises:
        AssertionError: If provider not initialized (programming error)
    """
    assert _provider is not None, (
        "AI Provider not initialized! This is a programming error. "
        "Call initialize_provider(config_path) first."
    )

    return _provider.get_embedding_metadata()
```

#### 2.2 Update `storage.py`

**Modify `get_or_create_collection()` to accept embedding metadata:**

```python
def get_or_create_collection(
    client: chromadb.Client,
    collection_name: str = "bear_notes",
    description: str = "",
    embedding_metadata: Optional[Dict[str, Any]] = None
) -> chromadb.Collection:
    """
    Get or create ChromaDB collection with embedding metadata.

    Args:
        client: ChromaDB client instance
        collection_name: Name of the collection
        description: Human-readable description of collection contents
        embedding_metadata: Metadata from AIProvider.get_embedding_metadata()
                           Contains: embedding_model, embedding_dimension,
                                    embedding_provider, embedding_base_url

    Returns:
        ChromaDB collection instance
    """
    from datetime import datetime, timezone

    # Build base metadata
    metadata = {
        "hnsw:space": "cosine",
        "version": "1.0",
        "created_at": datetime.now(timezone.utc).isoformat(),
    }

    if description:
        metadata["description"] = description

    # Merge in embedding metadata from AI provider
    if embedding_metadata:
        metadata.update(embedding_metadata)

    collection = client.get_or_create_collection(
        name=collection_name,
        metadata=metadata
    )

    return collection
```

#### 2.3 Update `full_pipeline.py`

**Add config parameter and wire metadata flow:**

```python
from embedding import initialize_provider, generate_embeddings_batch, get_embedding_metadata, AIProviderError
from storage import get_or_create_collection

def main():
    parser = argparse.ArgumentParser(
        description="Complete RAG pipeline: JSON â†’ Chunks â†’ Embeddings â†’ ChromaDB"
    )

    parser.add_argument("input_json", help="Path to input JSON file")

    # NEW: Config file argument (required)
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to configuration file (includes AI provider settings)"
    )

    parser.add_argument("--collection-name", default="bear_notes")
    parser.add_argument("--chunk-size", type=int, default=1200)
    # ... other arguments

    args = parser.parse_args()

    try:
        # Initialize AI provider from config
        print("ðŸ¤– Initializing AI provider...")
        provider_status = initialize_provider(args.config)

        if not provider_status["available"]:
            print(f"ðŸ’¥ AI provider unavailable: {provider_status['error']}")
            sys.exit(1)

        print(f"âœ… Provider ready: {provider_status['provider']} - {provider_status['embedding_model']}")
        print(f"   Embedding dimension: {provider_status['embedding_dim']}")

        # ... load notes, create chunks ...

        # Get embedding metadata from provider
        embedding_metadata = get_embedding_metadata()

        # Create ChromaDB collection with embedding metadata
        collection = get_or_create_collection(
            client,
            collection_name=args.collection_name,
            description=collection_description,
            embedding_metadata=embedding_metadata
        )

        # Generate embeddings using configured provider
        chunks_with_embeddings = generate_embeddings_batch(chunks)

        # ... continue pipeline ...

    except AIProviderError as e:
        print(f"ðŸ’¥ AI provider error: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"ðŸ’¥ Pipeline error: {e}")
        sys.exit(1)
```

#### 2.4 Update MCP Server

**New MCP Server Structure:**

```python
from mcp.server.fastmcp import FastMCP
from ai_provider import AIProvider, load_provider_from_config, AIProviderError
import chromadb

# Initialize MCP and AI provider
mcp = FastMCP("Markdown Notes Search")
ai_provider: AIProvider = None
chromadb_client: chromadb.Client = None


@mcp.tool()
def list_knowledge_bases() -> List[Dict[str, Any]]:
    """List all available knowledge bases with their metadata."""
    collections = chromadb_client.list_collections()

    result = []
    for coll in collections:
        metadata = coll.metadata or {}
        result.append({
            "name": coll.name,
            "description": metadata.get("description", "No description"),
            "embedding_model": metadata.get("embedding_model", "unknown"),
            "embedding_dimension": metadata.get("embedding_dimension", "unknown"),
            "created_at": metadata.get("created_at", "unknown")
        })

    return result


@mcp.tool()
def search_knowledge_base(
    query: str,
    collection_name: str,
    max_results: int = 5,
    context_mode: Literal["chunk_only", "enhanced", "full_note"] = "enhanced"
) -> List[Dict[str, Any]]:
    """Search a specific knowledge base."""
    global ai_provider, chromadb_client

    # Get collection
    collection = chromadb_client.get_collection(collection_name)
    collection_meta = collection.metadata or {}

    # CRITICAL: Validate embedding compatibility
    collection_model = collection_meta.get("embedding_model")
    current_model = ai_provider.config.embedding_model

    if collection_model != current_model:
        raise ValueError(
            f"Embedding model mismatch!\n"
            f"Collection '{collection_name}' uses: {collection_model}\n"
            f"Current MCP server uses: {current_model}\n"
            f"Please use the same embedding model for consistency."
        )

    # Generate query embedding using configured provider
    query_embedding = ai_provider.generate_embedding(query)

    # Search ChromaDB
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=max_results,
        include=["documents", "metadatas", "distances"]
    )

    # Format and return results
    # ... format results based on context_mode ...

    return formatted_results


if __name__ == "__main__":
    import os

    # Load configuration
    config_path = os.environ.get("MCP_CONFIG_PATH", "config.json")

    try:
        # Initialize AI provider
        print(f"ðŸ¤– Loading AI provider from {config_path}...")
        ai_provider = load_provider_from_config(config_path)

        # Verify provider availability
        status = ai_provider.check_availability()
        if not status["available"]:
            raise RuntimeError(f"AI provider unavailable: {status['error']}")

        print(f"âœ… AI Provider ready: {status['provider']} - {status['embedding_model']}")

        # Initialize ChromaDB client
        # Load chromadb_path from same config
        with open(config_path) as f:
            config = json.load(f)
        chromadb_path = config.get("chromadb_path", "./chromadb_data")

        chromadb_client = chromadb.PersistentClient(path=chromadb_path)
        print(f"âœ… ChromaDB client ready: {chromadb_path}")

        # Start MCP server
        print("ðŸš€ Starting MCP server...")
        mcp.run()

    except Exception as e:
        print(f"ðŸ’¥ MCP server initialization failed: {e}")
        raise
```

---

## Security Considerations

### API Key Management - DEFINITIVE APPROACH

**API keys are NEVER stored in files. Period.**

#### Environment Variable Resolution

**Configuration files reference environment variables:**

```json
{
  "ai_provider": {
    "provider_type": "openai",
    "embedding_model": {
      "model": "text-embedding-3-small",
      "api_key": "${OPENAI_API_KEY}"
    }
  }
}
```

**Environment variables must be set before running:**

```bash
# Set API keys in shell
export OPENAI_API_KEY="sk-..."
export GEMINI_API_KEY="AIza..."

# Then run pipeline
cd markdown-notes-cag-data-creator
python full_pipeline.py --config config.json --verbose input.json
```

**The `AIProviderConfig._resolve_api_key()` method handles resolution:**

```python
def _resolve_api_key(self, api_key: Optional[str]) -> Optional[str]:
    """Resolve API key from environment variables if needed"""
    if api_key and api_key.startswith("${") and api_key.endswith("}"):
        env_var = api_key[2:-1]  # Extract "OPENAI_API_KEY"
        value = os.environ.get(env_var)
        if value is None:
            raise ValueError(f"Environment variable '{env_var}' not set")
        return value
    return api_key
```

#### Git Repository Structure

**Config files CAN be committed:**

```gitignore
# .gitignore - MINIMAL APPROACH
# Config files are safe to commit (they reference env vars, not secrets)

# Only ignore local overrides if needed
config.local.json
```

**Provide example templates:**

```json
// config.example.json (committed to git)
{
  "chromadb_path": "../chromadb_data",
  "default_max_results": 5,

  "ai_provider": {
    "provider_type": "ollama",

    "embedding_model": {
      "model": "mxbai-embed-large:latest",
      "base_url": "http://localhost:11434",
      "api_key": null
    },

    "llm_model": {
      "model": "llama3.1:8b",
      "base_url": "http://localhost:11434",
      "api_key": null
    }
  }
}
```

---

## Provider-Specific Considerations

### Embedding Dimension Compatibility

**CRITICAL: Different models have different embedding dimensions!**

| Provider | Model                  | Dimensions |
| -------- | ---------------------- | ---------- |
| Ollama   | mxbai-embed-large      | 1024       |
| OpenAI   | text-embedding-3-small | 1536       |
| OpenAI   | text-embedding-3-large | 3072       |
| Google   | text-embedding-004     | 768        |

**Solution: Metadata validation enforced at query time**

**Storage (at collection creation):**

```python
# In full_pipeline.py - metadata flows from provider â†’ ChromaDB
embedding_metadata = get_embedding_metadata()  # From ai_provider

collection = get_or_create_collection(
    client,
    collection_name="my_notes",
    description="Personal notes",
    embedding_metadata=embedding_metadata  # Stored in ChromaDB
)

# Resulting collection.metadata:
# {
#     "embedding_model": "mxbai-embed-large:latest",
#     "embedding_dimension": 1024,
#     "embedding_provider": "ollama",
#     "embedding_base_url": "http://localhost:11434"
# }
```

**Validation (at query time in MCP server):**

```python
def search_knowledge_base(query: str, collection_name: str):
    collection = chromadb_client.get_collection(collection_name)
    collection_meta = collection.metadata

    # Hard error on mismatch
    if collection_meta["embedding_model"] != ai_provider.config.embedding_model:
        raise ValueError(
            f"Embedding model mismatch!\n"
            f"Collection uses: {collection_meta['embedding_model']}\n"
            f"Current provider uses: {ai_provider.config.embedding_model}\n"
            f"Please use the same embedding model for consistency."
        )

    # Proceed with search...
```

### LiteLLM Model Naming Conventions

**Model names come directly from config - LiteLLM auto-detects provider:**

```python
# Configuration specifies exact model name
# LiteLLM auto-detects provider from model string format

# Ollama models (local):
"model": "mxbai-embed-large:latest"  # Auto-detected as Ollama

# OpenAI models:
"model": "text-embedding-3-small"  # Auto-detected as OpenAI

# Gemini models (may require explicit prefix):
"model": "text-embedding-004"
# OR
"model": "gemini/text-embedding-004"  # Explicit provider prefix

# The `provider_type` in config is for metadata/documentation only
# LiteLLM uses model string format for actual routing
```

**No transformations needed** - pass model name directly from config to LiteLLM.

### Cost Considerations

**Local (Ollama):**

- âœ… Free
- âœ… Unlimited usage
- âš ï¸ Requires local GPU/CPU
- âš ï¸ Slower inference

**OpenAI:**

- ðŸ’° `text-embedding-3-small`: $0.02 per 1M tokens
- ðŸ’° `text-embedding-3-large`: $0.13 per 1M tokens
- âœ… Fast inference
- âš ï¸ Requires internet
- âš ï¸ API rate limits

**Google Gemini:**

- ðŸ’° `text-embedding-004`: Free tier available, then $0.025 per 1M chars
- âœ… Fast inference
- âš ï¸ Requires internet

**Recommendation:** Start with Ollama for development, switch to cloud providers for production if:

1. Need faster inference
2. Don't want to manage local models
3. Want multi-region availability

---

## Testing Strategy

**Tests will be implemented AFTER the core functionality is working.**

### Testing Priority (Post-Implementation)

#### Phase 1: Core Functionality (Required)

1. âœ… Test `ai_provider.py` with Ollama (local, no API key needed)
2. âœ… Test metadata flow from config â†’ ChromaDB
3. âœ… Test embedding dimension validation (mismatch detection)
4. âœ… Test provider initialization failures
5. âœ… Test assertion errors for uninitialized provider

#### Phase 2: Cloud Providers (Optional - if API keys available)

6. â­• Test with OpenAI (requires `OPENAI_API_KEY`)
7. â­• Test with Gemini (requires `GEMINI_API_KEY`)
8. â­• Test environment variable resolution
9. â­• Test missing environment variable errors

#### Phase 3: Integration (Final)

10. âœ… Full pipeline test (JSON â†’ ChromaDB with Ollama)
11. âœ… MCP server query test with embedding validation
12. â­• Performance benchmarking (Ollama vs cloud providers)

### Example Unit Tests

```python
# test_ai_provider.py

def test_ollama_provider():
    """Test Ollama provider initialization and embedding generation"""
    config = AIProviderConfig(
        provider_type="ollama",
        embedding_model="mxbai-embed-large:latest",
        llm_model="llama3.1:8b",
        base_url="http://localhost:11434"
    )
    provider = AIProvider(config)

    embedding = provider.generate_embedding("test")
    assert len(embedding) == 1024  # mxbai dimension
    assert isinstance(embedding, list)


def test_environment_variable_resolution():
    """Test API key resolution from environment variables"""
    import os
    os.environ["TEST_API_KEY"] = "sk-test123"

    config = AIProviderConfig(
        provider_type="openai",
        embedding_model="text-embedding-3-small",
        llm_model="gpt-4o-mini",
        api_key="${TEST_API_KEY}"
    )

    assert config.api_key == "sk-test123"


def test_missing_environment_variable():
    """Test error handling for missing environment variables"""
    with pytest.raises(ValueError, match="Environment variable.*not set"):
        config = AIProviderConfig(
            provider_type="openai",
            embedding_model="text-embedding-3-small",
            llm_model="gpt-4o-mini",
            api_key="${NONEXISTENT_KEY}"
        )


def test_metadata_generation():
    """Test embedding metadata generation"""
    provider = create_test_ollama_provider()
    metadata = provider.get_embedding_metadata()

    assert "embedding_model" in metadata
    assert "embedding_dimension" in metadata
    assert "embedding_provider" in metadata
    assert metadata["embedding_dimension"] == 1024
```

---

## Migration Strategy

### Step-by-Step Implementation

#### Step 1: Add Dependencies

```bash
cd markdown-notes-cag-data-creator
source ../.venv/bin/activate
pip install litellm
```

#### Step 2: Create Configuration Files

```bash
# Pipeline config
cat > markdown-notes-cag-data-creator/config.json << EOF
{
  "chromadb_path": "../chromadb_data",
  "default_max_results": 5,
  "ai_provider": {
    "provider_type": "ollama",
    "embedding_model": {
      "model": "mxbai-embed-large:latest",
      "base_url": "http://localhost:11434"
    },
    "llm_model": {
      "model": "llama3.1:8b",
      "base_url": "http://localhost:11434"
    }
  }
}
EOF

# MCP server config (copy)
cp markdown-notes-cag-data-creator/config.json markdown-notes-mcp-server/config.json
```

#### Step 3: Implement `ai_provider.py`

- Create the abstraction layer module
- Implement all methods as specified above

#### Step 4: Refactor `embedding.py`

- Replace direct Ollama calls with abstraction layer
- Add assertion-based initialization checks
- Add `get_embedding_metadata()` function

#### Step 5: Update `storage.py`

- Modify `get_or_create_collection()` to accept `embedding_metadata` parameter
- Merge metadata into collection metadata

#### Step 6: Update `full_pipeline.py`

- Add `--config` required parameter
- Initialize provider at startup
- Get metadata and pass to collection creation

#### Step 7: Update MCP Server

- Load provider from config
- Add embedding validation in `search_knowledge_base()`
- Display provider info at startup

#### Step 8: Testing (Post-Implementation)

- Test with Ollama
- Test metadata flow
- Test validation errors
- (Optional) Test cloud providers

---

## Performance Benchmarks

### Expected Performance by Provider

**Embedding Generation (1000 chunks, ~500 chars each):**

| Provider      | Model                  | Time  | Cost  | Notes           |
| ------------- | ---------------------- | ----- | ----- | --------------- |
| Ollama (CPU)  | mxbai-embed-large      | ~120s | $0    | MacBook Pro M1  |
| Ollama (GPU)  | mxbai-embed-large      | ~25s  | $0    | NVIDIA RTX 3090 |
| OpenAI        | text-embedding-3-small | ~15s  | $0.01 | Batch API       |
| OpenAI        | text-embedding-3-large | ~20s  | $0.06 | Batch API       |
| Google Gemini | text-embedding-004     | ~18s  | $0.01 | Free tier       |

**LLM Validation (1 description check):**

| Provider      | Model            | Time  | Cost     |
| ------------- | ---------------- | ----- | -------- |
| Ollama        | llama3.1:8b      | ~2s   | $0       |
| OpenAI        | gpt-4o-mini      | ~0.5s | $0.0001  |
| Google Gemini | gemini-1.5-flash | ~0.8s | $0.00001 |

---

## Summary and Recommendations

### Implementation Status: READY TO PROCEED

**All major design decisions have been finalized.**

âœ… **Finalized:**

1. Two-config architecture (pipeline + MCP server)
2. Environment variable-only API key management
3. Hard error on embedding dimension mismatch
4. Assertion-based provider initialization checks
5. No backward compatibility requirements
6. Config â†’ Provider â†’ ChromaDB metadata flow
7. Post-implementation testing strategy

### Recommended Implementation Order

1. **Week 1: Foundation**

   - Implement `ai_provider.py` (Phase 1)
   - Create config files and templates
   - Add LiteLLM dependency

2. **Week 2: Pipeline Integration**

   - Refactor `embedding.py` (Phase 2.1)
   - Update `storage.py` (Phase 2.2)
   - Update `full_pipeline.py` (Phase 2.3)
   - Test metadata flow

3. **Week 3: MCP Server Integration**

   - Update MCP server (Phase 2.4)
   - Add embedding validation
   - Test with Claude Desktop

4. **Week 4: Testing & Documentation**
   - Comprehensive testing (all phases)
   - Update CLAUDE.md
   - Create usage examples
   - (Optional) Test cloud providers

### Key Success Criteria

- âœ… Ollama works identically to current implementation
- âœ… Config files can switch providers without code changes
- âœ… Embedding mismatch errors are clear and actionable
- âœ… API keys never appear in version control
- âœ… Metadata flows correctly through entire pipeline

---

## Next Actions

### Immediate Steps

1. **Install LiteLLM:**

   ```bash
   pip install litellm
   ```

2. **Create `ai_provider.py`:**

   - Copy implementation from Phase 1 above
   - Test basic functionality with Ollama

3. **Create config files:**

   - `markdown-notes-cag-data-creator/config.json`
   - `markdown-notes-mcp-server/config.json`
   - Both using Ollama configuration

4. **Begin refactoring:**
   - Start with `embedding.py`
   - Then `storage.py`
   - Then `full_pipeline.py`
   - Finally MCP server

---

## Conclusion

**The AI provider abstraction layer is implementation-ready.** All architectural decisions have been made, and the design is clean, secure, and maintainable.

**Key Benefits:**

1. âœ… Flexibility to switch AI providers via configuration
2. âœ… Security-first approach (no secrets in files)
3. âœ… Strong validation (embedding compatibility checks)
4. âœ… Minimal code changes (~400 lines total)
5. âœ… Clean metadata flow throughout pipeline

**Recommendation: Proceed with implementation following the plan above.**
