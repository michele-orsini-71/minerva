# AI Provider Abstraction Layer Analysis (v3)

## Switching Between Ollama and OpenAI-compatible Services

**Version:** 3.0
**Date:** 2025-10-07
**Status:** Implementation-Ready
**Changes from v2:** Corrected configuration architecture - pipeline config is per-run, MCP config is minimal

---

## Important Design Constraint

‚ö†Ô∏è **CRITICAL:** The embedding of notes data (CAG data creator) and the embedding of the user question (MCP server) **MUST** be done with the **SAME** engine.

**Implementation requirement:** The embedding engine specification must:

1. Appear in the ChromaDB collection metadata at creation time (stored during pipeline run)
2. Be validated by the MCP server at query time to verify embedding engine compatibility
3. ChromaDB collection metadata is the **source of truth** for which AI provider to use

---

## Executive Summary

**TL;DR: YES, it's highly feasible to add AI provider abstraction to support both Ollama (local) and OpenAI/Gemini/etc. (remote) services.** The codebase is well-structured for this change, requiring minimal refactoring. Using LiteLLM as an abstraction layer would provide unified access to 100+ AI providers with a single API.

**Key architectural insight:** Each pipeline run has its own configuration file that includes AI provider settings. These settings are stored in ChromaDB collection metadata. The MCP server dynamically discovers available collections at startup by reading metadata and testing provider availability.

---

## Implementation Decisions (FINALIZED)

The following architectural decisions have been finalized:

### 1. Configuration File Strategy ‚úì **[CORRECTED in v3]**

**Pipeline Configuration: PER-RUN**

- Each collection creation has its **own config file** (e.g., `work-notes-config.json`)
- Config includes **everything**: collection settings + AI provider settings
- Users can name config files however they want
- AI provider settings are **embedded in ChromaDB metadata**

**MCP Server Configuration: MINIMAL**

- Single global config file: `markdown-notes-mcp-server/config.json`
- Contains **only**: `chromadb_path`
- **NO** AI provider configuration (discovered from collection metadata)

**Secret Coupling (Known Limitation):**

- Pipeline config references env vars: `"api_key": "${OPENAI_API_KEY}"`
- MCP server must have **same env var names** available
- This is a **naming contract** between pipeline and MCP
- Alternative approaches were considered but this is the simplest

### 2. Metadata Storage ‚úì **[CORRECTED in v3]**

- **Store API key template reference** (e.g., `"${OPENAI_API_KEY}"`) in metadata
- MCP server resolves env vars at startup when testing collection availability
- Full AI provider config is stored in each collection's metadata

### 3. MCP Collection Availability ‚úì **[NEW in v3]**

- **Test actual embedding generation** at startup
- For each collection, try `provider.generate_embedding("test")`
- This validates: provider availability, API keys, network connectivity, model availability
- More thorough than just instantiation, catches real-world issues

### 4. MCP Collection Listing ‚úì **[NEW in v3]**

- `list_knowledge_bases()` returns **only available collections**
- Unavailable collections are **not exposed** to AI agents
- Full details (including unavailable) logged to console for admin visibility
- Prevents AI confusion from seeing unavailable collections

### 5. Backward Compatibility ‚úì

- Collections without AI provider metadata are **treated as unavailable**
- Forces re-indexing with new pipeline (safe approach)
- Only one ChromaDB database exists (current one will be recreated)

### 6. Embedding Dimension Validation ‚úì

- **Hard error on mismatch** - No warnings, complete blocking
- Validation occurs at query time in MCP server
- Clear error message explaining the mismatch

### 7. Provider Initialization ‚úì

- **Assertion-based protection** for uninitialized provider in pipeline
- Programming error (not user error) - should never happen in production
- Clear error messages for debugging

### 8. API Key Security ‚úì

- **Environment variable injection only**
- Config files reference env vars: `"${OPENAI_API_KEY}"`
- Template references stored in metadata (not actual keys)
- Resolution happens at runtime via `AIProviderConfig._resolve_api_key()`

### 9. LiteLLM Model Naming ‚úì

- **Model names come directly from config**
- LiteLLM auto-detects provider from model string format
- No prefix transformations needed
- `provider_type` is for metadata only

### 10. Testing Strategy ‚úì

- **Post-implementation testing**
- Priority: Ollama first, cloud providers optional
- No CI/CD constraints for API key exposure

---

## Current AI Usage Points

I've identified **THREE distinct AI operations** across the pipeline and MCP server:

### 1. **Vector Embedding Creation** (Primary Use)

**Current Implementation:**

- **Location:** `markdown-notes-cag-data-creator/embedding.py`
- **Model:** `mxbai-embed-large:latest` via Ollama
- **Usage:** Converting text chunks to 1024-dimensional vectors
- **Frequency:** Batch processing during pipeline runs (hundreds to thousands of calls)

**Code Pattern:**

```python
from ollama import embeddings as ollama_embeddings

def generate_embedding(text: str, model: str = EMBED_MODEL):
    response = ollama_embeddings(model=model, prompt=text)
    vector = np.array(response['embedding'])
    normalized = l2_normalize(vector.reshape(1, -1))
    return normalized.flatten().tolist()
```

### 2. **Collection Description Validation** (AI-Enhanced QA)

**Future Implementation (PRD Spec):**

- **Location:** Will be added to `markdown-notes-cag-data-creator/full_pipeline.py`
- **Model:** Configured LLM (e.g., `llama3.1:8b`)
- **Usage:** Validating collection descriptions for AI-friendliness (scoring 0-10)
- **Frequency:** Once per collection creation (low volume)

**Expected Code Pattern (from PRD):**

```python
# TIER 2: Optional AI validation
ai_result = validate_with_ai(description)  # Uses configured LLM
if not ai_result["passed"]:
    issues.append(f"AI validation failed (score: {ai_result['score']}/10)")
```

### 3. **Query Embedding Generation** (Search-time)

**Current Implementation:**

- **Location:** `test-files/chromadb_query_client.py` (will move to MCP server)
- **Model:** Same embedding model as pipeline
- **Usage:** Converting user queries to embeddings for semantic search
- **Frequency:** Per user query (interactive, low-to-medium volume)

**Code Pattern:**

```python
def generate_query_embedding(query: str):
    embedding = generate_embedding(query.strip())  # Reuses embedding.py
    return embedding
```

---

## Proposed Abstraction Architecture

### Solution: LiteLLM Abstraction Layer

**LiteLLM** is the ideal choice for this use case:

- ‚úÖ Unified API for 100+ providers (OpenAI, Anthropic, Google Gemini, Azure, Ollama, etc.)
- ‚úÖ **Supports both embeddings AND completions** (covers all 3 use cases)
- ‚úÖ Drop-in replacement for OpenAI SDK
- ‚úÖ Automatic fallback/retry logic
- ‚úÖ Standardized response format across providers
- ‚úÖ Local caching support

### Configuration Architecture

#### Per-Run Pipeline Configuration

**Each pipeline run has its own configuration file** containing both collection settings and AI provider settings.

**File Structure:**

```
project-root/
‚îú‚îÄ‚îÄ configs/                          # User-managed config files
‚îÇ   ‚îú‚îÄ‚îÄ work-notes-openai.json       # OpenAI for work notes
‚îÇ   ‚îú‚îÄ‚îÄ personal-notes-ollama.json   # Ollama for personal notes
‚îÇ   ‚îî‚îÄ‚îÄ research-gemini.json         # Gemini for research
‚îú‚îÄ‚îÄ markdown-notes-cag-data-creator/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ markdown-notes-mcp-server/
    ‚îú‚îÄ‚îÄ config.json                  # Minimal MCP config
    ‚îî‚îÄ‚îÄ ...
```

**Pipeline Configuration Schema (Complete Example):**

```json
{
  "collection_name": "work_notes",
  "description": "Work-related technical notes covering Python, Docker, and cloud infrastructure",
  "forceRecreate": false,
  "skipAiValidation": false,
  "chromadb_path": "../chromadb_data",
  "json_file": "../test-data/work-notes.json",

  "ai_provider": {
    "provider_type": "openai",

    "embedding_model": {
      "model": "text-embedding-3-small",
      "api_key": "${OPENAI_API_KEY}"
    },

    "llm_model": {
      "model": "gpt-4o-mini",
      "api_key": "${OPENAI_API_KEY}"
    }
  }
}
```

**Ollama Example (Local):**

```json
{
  "collection_name": "personal_notes",
  "description": "Personal knowledge base with programming tips and tutorials",
  "forceRecreate": false,
  "skipAiValidation": false,
  "chromadb_path": "../chromadb_data",
  "json_file": "../test-data/personal-notes.json",

  "ai_provider": {
    "provider_type": "ollama",

    "embedding_model": {
      "model": "mxbai-embed-large:latest",
      "base_url": "http://localhost:11434",
      "api_key": null
    },

    "llm_model": {
      "model": "llama3.1:8b",
      "base_url": "http://localhost:11434",
      "api_key": null
    }
  }
}
```

**Gemini Example:**

```json
{
  "collection_name": "research_notes",
  "description": "Academic research papers and scientific notes",
  "forceRecreate": false,
  "skipAiValidation": false,
  "chromadb_path": "../chromadb_data",
  "json_file": "../test-data/research-notes.json",

  "ai_provider": {
    "provider_type": "gemini",

    "embedding_model": {
      "model": "text-embedding-004",
      "api_key": "${GEMINI_API_KEY}"
    },

    "llm_model": {
      "model": "gemini-1.5-flash",
      "api_key": "${GEMINI_API_KEY}"
    }
  }
}
```

#### MCP Server Configuration (Minimal)

**The MCP server config is extremely simple - just ChromaDB path:**

```json
{
  "chromadb_path": "../chromadb_data"
}
```

**All AI provider information is discovered from collection metadata at startup.**

---

## Data Flow Architecture

### Pipeline Execution Flow

```
1. User creates config file (e.g., work-notes-openai.json)
2. User exports environment variables:
   export OPENAI_API_KEY="sk-..."
3. User runs pipeline:
   python full_pipeline.py --config configs/work-notes-openai.json

Pipeline Processing:
4. Load config file
5. Resolve env vars (${OPENAI_API_KEY} ‚Üí actual key)
6. Initialize AIProvider with resolved config
7. Test provider availability
8. Get embedding metadata from provider
9. Create/update ChromaDB collection with FULL metadata:
   {
     "embedding_model": "text-embedding-3-small",
     "embedding_provider": "openai",
     "embedding_dimension": 1536,
     "embedding_api_key_ref": "${OPENAI_API_KEY}",  ‚Üê Template stored!
     "embedding_base_url": null,
     "llm_model": "gpt-4o-mini",
     "description": "Work-related technical notes...",
     "created_at": "2025-10-07T10:30:00Z",
     ...
   }
10. Generate embeddings and insert chunks
11. Pipeline complete ‚úì
```

### MCP Server Startup Flow

```
1. MCP server starts
2. Load minimal config (chromadb_path only)
3. Connect to ChromaDB
4. List all collections

For each collection:
5. Read collection.metadata
6. Extract AI provider config from metadata:
   - embedding_model
   - embedding_provider
   - embedding_api_key_ref (e.g., "${OPENAI_API_KEY}")
   - embedding_base_url
7. Build AIProviderConfig from metadata
8. Resolve env vars (${OPENAI_API_KEY} ‚Üí actual key from environment)
9. Try to instantiate AIProvider
10. Test with provider.generate_embedding("test")
11. If successful ‚Üí Mark collection as AVAILABLE
    If failed ‚Üí Mark collection as UNAVAILABLE (log reason)

After processing all collections:
12. Log summary to console:
    ‚úÖ Available: work_notes (openai/text-embedding-3-small)
    ‚úÖ Available: personal_notes (ollama/mxbai-embed-large)
    ‚ö†Ô∏è  Unavailable: research_notes (gemini/text-embedding-004)
        Reason: Environment variable GEMINI_API_KEY not set

13. If zero collections available ‚Üí EXIT WITH ERROR
14. If some collections available ‚Üí START SERVER
15. Store map: {collection_name ‚Üí AIProvider} for available collections
```

### MCP Query Flow

```
1. AI agent calls search_knowledge_base("docker networking", "work_notes")
2. MCP checks if "work_notes" in available_collections
3. If NOT available ‚Üí Return error: "Collection unavailable: API key not set"
4. Get AIProvider for "work_notes" from stored map
5. Generate query embedding: provider.generate_embedding("docker networking")
6. Query ChromaDB with embedding
7. Validate embedding dimension matches (from metadata)
8. Return results to AI agent
```

---

## Implementation Plan

### Phase 1: Create AI Provider Abstraction Layer

**New Module:** `markdown-notes-cag-data-creator/ai_provider.py`

```python
#!/usr/bin/env python3
"""
AI Provider Abstraction Layer
Supports multiple embedding and LLM providers via LiteLLM
"""

import os
import json
from typing import List, Dict, Any, Optional, Literal
from litellm import embedding, completion
import numpy as np


ProviderType = Literal["ollama", "openai", "gemini", "azure", "anthropic"]


class AIProviderConfig:
    """Configuration for AI provider"""
    def __init__(
        self,
        provider_type: ProviderType,
        embedding_model: str,
        llm_model: str,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        api_key_ref: Optional[str] = None  # NEW: Store original reference
    ):
        self.provider_type = provider_type
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        self.base_url = base_url
        self.api_key_ref = api_key_ref or api_key  # Store original reference
        self.api_key = self._resolve_api_key(api_key)

    def _resolve_api_key(self, api_key: Optional[str]) -> Optional[str]:
        """
        Resolve API key from environment variables if needed.

        Supports syntax: "${ENV_VAR_NAME}" which gets resolved to os.environ["ENV_VAR_NAME"]
        """
        if api_key and api_key.startswith("${") and api_key.endswith("}"):
            env_var = api_key[2:-1]  # Extract "OPENAI_API_KEY" from "${OPENAI_API_KEY}"
            value = os.environ.get(env_var)
            if value is None:
                raise ValueError(
                    f"Environment variable '{env_var}' not set. "
                    f"Please export {env_var}=<your-api-key>"
                )
            return value
        return api_key


class AIProvider:
    """Unified interface for AI operations across providers"""

    def __init__(self, config: AIProviderConfig):
        self.config = config

    def generate_embedding(
        self,
        text: str,
        normalize: bool = True
    ) -> List[float]:
        """
        Generate embedding using configured provider.

        Args:
            text: Input text to embed
            normalize: Apply L2 normalization (for cosine similarity)

        Returns:
            Embedding vector as list of floats
        """
        try:
            response = embedding(
                model=self.config.embedding_model,
                input=[text],
                api_key=self.config.api_key,
                api_base=self.config.base_url
            )

            vector = np.array(response.data[0]['embedding'], dtype=np.float32)

            if normalize:
                vector = self._l2_normalize(vector)

            return vector.tolist()

        except Exception as e:
            raise AIProviderError(f"Embedding generation failed: {e}")

    def generate_embeddings_batch(
        self,
        texts: List[str],
        normalize: bool = True,
        progress_callback: Optional[callable] = None
    ) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        embeddings = []

        for i, text in enumerate(texts):
            if progress_callback:
                progress_callback(i, len(texts))

            emb = self.generate_embedding(text, normalize=normalize)
            embeddings.append(emb)

        if progress_callback:
            progress_callback(len(texts), len(texts))

        return embeddings

    def validate_description(
        self,
        description: str,
        threshold: int = 7
    ) -> Dict[str, Any]:
        """
        Use LLM to validate collection description quality.

        Args:
            description: Collection description to validate
            threshold: Minimum score to pass (0-10)

        Returns:
            {"passed": bool, "score": int, "reason": str}
        """
        prompt = f"""
        Rate this collection description on a scale of 0-10 for AI agent usability.
        A good description clearly explains:
        1. What content is in the collection
        2. When an AI should query this collection
        3. What types of questions it answers

        Description to rate:
        "{description}"

        Respond in JSON format:
        {{
            "score": <0-10>,
            "reason": "<brief explanation>"
        }}
        """

        try:
            response = completion(
                model=self.config.llm_model,
                messages=[{"role": "user", "content": prompt}],
                api_key=self.config.api_key,
                api_base=self.config.base_url,
                response_format={"provider_type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            score = result.get("score", 0)

            return {
                "passed": score >= threshold,
                "score": score,
                "reason": result.get("reason", "No reason provided")
            }

        except Exception as e:
            raise AIProviderError(f"Description validation failed: {e}")

    def get_embedding_metadata(self) -> Dict[str, Any]:
        """
        Get embedding model metadata for storage in ChromaDB.

        This metadata will be used by MCP server to reconstruct
        the AIProvider at startup and validate compatibility.

        Returns:
            Dictionary with embedding configuration metadata
        """
        # Generate test embedding to determine actual dimension
        test_emb = self.generate_embedding("test", normalize=False)

        return {
            "embedding_model": self.config.embedding_model,
            "embedding_dimension": len(test_emb),
            "embedding_provider": self.config.provider_type,
            "embedding_base_url": self.config.base_url,
            "embedding_api_key_ref": self.config.api_key_ref,  # Store template!
            "llm_model": self.config.llm_model
        }

    def _l2_normalize(self, vector: np.ndarray) -> np.ndarray:
        """Apply L2 normalization for cosine similarity"""
        norm = np.linalg.norm(vector)
        if norm == 0:
            return vector
        return vector / norm

    def check_availability(self) -> Dict[str, Any]:
        """
        Check if provider is available and responsive.

        Tests actual embedding generation to validate:
        - Provider is reachable
        - API key is valid
        - Model is available
        - Network connectivity works
        """
        try:
            # Test with actual embedding generation
            test_embedding = self.generate_embedding("test", normalize=False)

            return {
                "available": True,
                "provider": self.config.provider_type,
                "embedding_model": self.config.embedding_model,
                "llm_model": self.config.llm_model,
                "embedding_dim": len(test_embedding)
            }
        except Exception as e:
            return {
                "available": False,
                "error": str(e)
            }


class AIProviderError(Exception):
    """Exception raised when AI provider operations fail"""
    pass


def load_provider_from_config(config_path: str) -> AIProvider:
    """
    Load AI provider from pipeline configuration file.

    Args:
        config_path: Path to pipeline config JSON file

    Returns:
        Initialized AIProvider instance
    """
    from pathlib import Path

    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    with open(config_file) as f:
        config = json.load(f)

    ai_config = config.get("ai_provider", {})

    provider_config = AIProviderConfig(
        provider_type=ai_config.get("type", "ollama"),
        embedding_model=ai_config["embedding"]["model"],
        llm_model=ai_config["llm"]["model"],
        base_url=ai_config["embedding"].get("base_url"),
        api_key=ai_config["embedding"].get("api_key")
    )

    return AIProvider(provider_config)


def load_provider_from_metadata(metadata: Dict[str, Any]) -> AIProvider:
    """
    Load AI provider from ChromaDB collection metadata.

    Used by MCP server to reconstruct AIProvider from stored metadata.

    Args:
        metadata: Collection metadata dictionary from ChromaDB

    Returns:
        Initialized AIProvider instance

    Raises:
        KeyError: If required metadata fields are missing
        ValueError: If environment variables are not set
    """
    provider_config = AIProviderConfig(
        provider_type=metadata["embedding_provider"],
        embedding_model=metadata["embedding_model"],
        llm_model=metadata.get("llm_model", ""),  # Optional
        base_url=metadata.get("embedding_base_url"),
        api_key=metadata.get("embedding_api_key_ref")  # Resolves env var
    )

    return AIProvider(provider_config)
```

---

### Phase 2: Refactor Existing Code

#### 2.1 Update `embedding.py`

**Complete refactored version:**

```python
#!/usr/bin/env python3
"""
Embedding generation module using configurable AI providers.
"""

from typing import List, Optional, Dict, Any
from ai_provider import AIProvider, load_provider_from_config, AIProviderError

# Global provider instance (loaded from config)
_provider: Optional[AIProvider] = None


def initialize_provider(config_path: str) -> Dict[str, Any]:
    """
    Initialize AI provider from pipeline configuration.

    Args:
        config_path: Path to pipeline configuration file

    Returns:
        Provider status dictionary with availability info

    Raises:
        AIProviderError: If provider initialization fails
        FileNotFoundError: If config file doesn't exist
    """
    global _provider
    _provider = load_provider_from_config(config_path)
    status = _provider.check_availability()

    if not status["available"]:
        raise AIProviderError(f"Provider unavailable: {status['error']}")

    return status


def generate_embedding(text: str) -> List[float]:
    """
    Generate embedding using configured AI provider.

    Args:
        text: Input text to embed

    Returns:
        Embedding vector as list of floats

    Raises:
        AssertionError: If provider not initialized (programming error)
        AIProviderError: If embedding generation fails
    """
    assert _provider is not None, (
        "AI Provider not initialized! This is a programming error. "
        "Call initialize_provider(config_path) before using embedding functions."
    )

    return _provider.generate_embedding(text)


def generate_embeddings_batch(
    texts: List[str],
    progress_callback=None
) -> List[List[float]]:
    """
    Generate batch embeddings using configured AI provider.

    Args:
        texts: List of texts to embed
        progress_callback: Optional callback function(current, total)

    Returns:
        List of embedding vectors

    Raises:
        AssertionError: If provider not initialized (programming error)
        AIProviderError: If embedding generation fails
    """
    assert _provider is not None, (
        "AI Provider not initialized! This is a programming error. "
        "Call initialize_provider(config_path) before using embedding functions."
    )

    return _provider.generate_embeddings_batch(texts, progress_callback=progress_callback)


def get_embedding_metadata() -> Dict[str, Any]:
    """
    Get embedding model metadata for ChromaDB collection.

    This metadata will be stored in ChromaDB and used by MCP server
    to reconstruct the AIProvider at startup.

    Returns:
        Metadata dictionary with embedding configuration

    Raises:
        AssertionError: If provider not initialized (programming error)
    """
    assert _provider is not None, (
        "AI Provider not initialized! This is a programming error. "
        "Call initialize_provider(config_path) first."
    )

    return _provider.get_embedding_metadata()


def validate_description(description: str, threshold: int = 7) -> Dict[str, Any]:
    """
    Validate collection description using configured LLM.

    Args:
        description: Collection description to validate
        threshold: Minimum score to pass (0-10)

    Returns:
        {"passed": bool, "score": int, "reason": str}

    Raises:
        AssertionError: If provider not initialized (programming error)
        AIProviderError: If validation fails
    """
    assert _provider is not None, (
        "AI Provider not initialized! This is a programming error."
    )

    return _provider.validate_description(description, threshold)
```

#### 2.2 Update `storage.py`

**Modify `get_or_create_collection()` to accept embedding metadata:**

```python
def get_or_create_collection(
    client: chromadb.Client,
    collection_name: str = "bear_notes",
    description: str = "",
    embedding_metadata: Optional[Dict[str, Any]] = None
) -> chromadb.Collection:
    """
    Get or create ChromaDB collection with embedding metadata.

    Args:
        client: ChromaDB client instance
        collection_name: Name of the collection
        description: Human-readable description of collection contents
        embedding_metadata: Metadata from AIProvider.get_embedding_metadata()
                           Contains: embedding_model, embedding_dimension,
                                    embedding_provider, embedding_base_url,
                                    embedding_api_key_ref, llm_model

    Returns:
        ChromaDB collection instance
    """
    from datetime import datetime, timezone

    # Build base metadata
    metadata = {
        "hnsw:space": "cosine",
        "version": "1.0",
        "created_at": datetime.now(timezone.utc).isoformat(),
    }

    if description:
        metadata["description"] = description

    # Merge in embedding metadata from AI provider
    # This is CRITICAL - MCP server will use this to reconstruct provider
    if embedding_metadata:
        metadata.update(embedding_metadata)

    collection = client.get_or_create_collection(
        name=collection_name,
        metadata=metadata
    )

    return collection
```

#### 2.3 Update `full_pipeline.py`

**Add config parameter and wire metadata flow:**

```python
#!/usr/bin/env python3
"""
Complete RAG pipeline: JSON ‚Üí Chunks ‚Üí Embeddings ‚Üí ChromaDB
"""

import argparse
import sys
from pathlib import Path

from json_loader import load_json_notes
from chunk_creator import create_chunks_for_notes
from embedding import (
    initialize_provider,
    generate_embeddings_batch,
    get_embedding_metadata,
    validate_description,
    AIProviderError
)
from storage import (
    initialize_chromadb_client,
    get_or_create_collection,
    insert_chunks_batch
)


def main():
    parser = argparse.ArgumentParser(
        description="Complete RAG pipeline: JSON ‚Üí Chunks ‚Üí Embeddings ‚Üí ChromaDB"
    )

    # Config file is now REQUIRED and contains everything
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to pipeline configuration file (JSON)"
    )

    parser.add_argument("--verbose", action="store_true", help="Verbose output")

    args = parser.parse_args()

    try:
        # Load configuration
        import json
        with open(args.config) as f:
            config = json.load(f)

        # Extract settings from config
        collection_name = config["collection_name"]
        description = config["description"]
        chromadb_path = config["chromadb_path"]
        json_file = config["json_file"]
        force_recreate = config.get("forceRecreate", False)
        skip_ai_validation = config.get("skipAiValidation", False)
        chunk_size = config.get("chunk_size", 1200)

        print(f"üìã Pipeline Configuration:")
        print(f"   Collection: {collection_name}")
        print(f"   Input: {json_file}")
        print(f"   ChromaDB: {chromadb_path}")
        print()

        # Initialize AI provider from config
        print("ü§ñ Initializing AI provider...")
        provider_status = initialize_provider(args.config)

        print(f"‚úÖ Provider ready: {provider_status['provider']}")
        print(f"   Embedding model: {provider_status['embedding_model']}")
        print(f"   Embedding dimension: {provider_status['embedding_dim']}")
        print(f"   LLM model: {provider_status['llm_model']}")
        print()

        # Optional: Validate description with LLM
        if not skip_ai_validation and description:
            print("üîç Validating collection description with LLM...")
            validation = validate_description(description)
            print(f"   Score: {validation['score']}/10")
            print(f"   Reason: {validation['reason']}")
            if not validation["passed"]:
                print("‚ö†Ô∏è  Description validation failed (continuing anyway)")
            print()

        # Load notes
        print(f"üìö Loading notes from {json_file}...")
        notes = load_json_notes(json_file)
        print(f"‚úÖ Loaded {len(notes)} notes")
        print()

        # Create chunks
        print(f"‚úÇÔ∏è  Creating chunks (target size: {chunk_size} chars)...")
        enriched_notes = create_chunks_for_notes(notes, target_chars=chunk_size)
        total_chunks = sum(len(note.get('chunks', [])) for note in enriched_notes)
        print(f"‚úÖ Created {total_chunks} chunks")
        print()

        # Generate embeddings
        print("üßÆ Generating embeddings...")
        all_chunks = []
        for note in enriched_notes:
            for chunk in note.get('chunks', []):
                all_chunks.append(chunk)

        chunk_texts = [chunk['content'] for chunk in all_chunks]

        def progress(current, total):
            if current % 10 == 0 or current == total:
                print(f"   Progress: {current}/{total} chunks")

        embeddings = generate_embeddings_batch(chunk_texts, progress_callback=progress)
        print(f"‚úÖ Generated {len(embeddings)} embeddings")
        print()

        # Get embedding metadata for ChromaDB
        embedding_metadata = get_embedding_metadata()

        if args.verbose:
            print("üìä Embedding metadata to store:")
            for key, value in embedding_metadata.items():
                print(f"   {key}: {value}")
            print()

        # Initialize ChromaDB
        print(f"üíæ Connecting to ChromaDB at {chromadb_path}...")
        client = initialize_chromadb_client(chromadb_path)

        # Create or get collection with metadata
        print(f"üì¶ Creating/updating collection '{collection_name}'...")
        collection = get_or_create_collection(
            client,
            collection_name=collection_name,
            description=description,
            embedding_metadata=embedding_metadata  # CRITICAL: Store AI provider info
        )
        print(f"‚úÖ Collection ready")
        print()

        # Insert chunks with embeddings
        print("üíæ Inserting chunks into ChromaDB...")
        chunks_with_embeddings = [
            {**chunk, 'embedding': emb}
            for chunk, emb in zip(all_chunks, embeddings)
        ]

        insert_chunks_batch(collection, chunks_with_embeddings, progress_callback=progress)
        print(f"‚úÖ Inserted {len(chunks_with_embeddings)} chunks")
        print()

        print("üéâ Pipeline complete!")
        print(f"   Collection: {collection_name}")
        print(f"   Total chunks: {len(chunks_with_embeddings)}")
        print(f"   Embedding model: {embedding_metadata['embedding_model']}")

    except AIProviderError as e:
        print(f"üí• AI provider error: {e}")
        sys.exit(1)
    except FileNotFoundError as e:
        print(f"üí• File not found: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"üí• Pipeline error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
```

#### 2.4 Update MCP Server

**Complete MCP server implementation with dynamic collection discovery:**

```python
#!/usr/bin/env python3
"""
MCP Server for Markdown Notes Search
Dynamically discovers available collections from ChromaDB metadata
"""

import os
import json
from typing import List, Dict, Any, Literal, Optional
from mcp.server.fastmcp import FastMCP
import chromadb

# Import AI provider abstraction
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "markdown-notes-cag-data-creator"))
from ai_provider import AIProvider, load_provider_from_metadata, AIProviderError


# Initialize MCP server
mcp = FastMCP("Markdown Notes Search")

# Global state
chromadb_client: Optional[chromadb.Client] = None
available_collections: Dict[str, Dict[str, Any]] = {}  # {name ‚Üí {provider, metadata}}


def initialize_server():
    """
    Initialize MCP server:
    1. Load ChromaDB path from config
    2. List all collections
    3. Test each collection's AI provider availability
    4. Build available_collections map
    """
    global chromadb_client, available_collections

    # Load minimal config
    config_path = os.environ.get("MCP_CONFIG_PATH", "config.json")
    config_path = os.path.join(os.path.dirname(__file__), config_path)

    print(f"üìã Loading MCP configuration from {config_path}...")
    with open(config_path) as f:
        config = json.load(f)

    chromadb_path = config["chromadb_path"]
    print(f"üíæ Connecting to ChromaDB at {chromadb_path}...")
    chromadb_client = chromadb.PersistentClient(path=chromadb_path)
    print(f"‚úÖ ChromaDB connected")
    print()

    # Discover collections
    print("üîç Discovering collections and testing AI provider availability...")
    print()

    collections = chromadb_client.list_collections()
    available_count = 0
    unavailable_count = 0

    for coll in collections:
        coll_name = coll.name
        metadata = coll.metadata or {}

        print(f"üì¶ Collection: {coll_name}")

        # Check if metadata contains AI provider info
        if "embedding_model" not in metadata or "embedding_provider" not in metadata:
            print(f"   ‚ö†Ô∏è  UNAVAILABLE: Missing AI provider metadata (created with old pipeline)")
            print(f"   ‚Üí Re-index this collection with new pipeline to enable")
            unavailable_count += 1
            print()
            continue

        # Extract AI provider info
        provider_type = metadata.get("embedding_provider", "unknown")
        embedding_model = metadata.get("embedding_model", "unknown")
        api_key_ref = metadata.get("embedding_api_key_ref")

        print(f"   Provider: {provider_type}")
        print(f"   Model: {embedding_model}")

        if api_key_ref:
            print(f"   API Key: {api_key_ref}")

        # Try to instantiate and test provider
        try:
            print(f"   Testing provider availability...", end=" ")

            provider = load_provider_from_metadata(metadata)

            # Test actual embedding generation
            test_embedding = provider.generate_embedding("test")

            print(f"‚úÖ AVAILABLE")
            print(f"   Embedding dimension: {len(test_embedding)}")

            # Store in available collections
            available_collections[coll_name] = {
                "provider": provider,
                "metadata": metadata,
                "collection": coll
            }
            available_count += 1

        except ValueError as e:
            # Environment variable not set
            print(f"‚ùå UNAVAILABLE")
            print(f"   Reason: {e}")
            unavailable_count += 1

        except Exception as e:
            # Other errors (network, API, etc.)
            print(f"‚ùå UNAVAILABLE")
            print(f"   Reason: {type(e).__name__}: {e}")
            unavailable_count += 1

        print()

    # Summary
    print("=" * 60)
    print(f"üìä Collection Discovery Summary:")
    print(f"   ‚úÖ Available: {available_count}")
    print(f"   ‚ùå Unavailable: {unavailable_count}")
    print(f"   üì¶ Total: {available_count + unavailable_count}")
    print("=" * 60)
    print()

    # Exit if no collections available
    if available_count == 0:
        print("üí• ERROR: No collections available!")
        print("   ‚Üí Check environment variables for API keys")
        print("   ‚Üí Ensure collections have AI provider metadata")
        print("   ‚Üí Run pipeline to create new collections")
        sys.exit(1)

    print(f"üöÄ MCP Server ready with {available_count} available collection(s)")
    print()


@mcp.tool()
def list_knowledge_bases() -> List[Dict[str, Any]]:
    """
    List all available knowledge bases.

    Returns only collections that are currently available
    (i.e., their AI provider can be instantiated and tested).

    Returns:
        List of available collections with metadata
    """
    global available_collections

    result = []
    for name, info in available_collections.items():
        metadata = info["metadata"]
        result.append({
            "name": name,
            "description": metadata.get("description", "No description"),
            "embedding_model": metadata.get("embedding_model", "unknown"),
            "embedding_provider": metadata.get("embedding_provider", "unknown"),
            "embedding_dimension": metadata.get("embedding_dimension", "unknown"),
            "created_at": metadata.get("created_at", "unknown")
        })

    return result


@mcp.tool()
def search_knowledge_base(
    query: str,
    collection_name: str,
    max_results: int = 5,
    context_mode: Literal["chunk_only", "enhanced", "full_note"] = "enhanced"
) -> List[Dict[str, Any]]:
    """
    Search a specific knowledge base with semantic search.

    Args:
        query: Search query text
        collection_name: Name of the collection to search
        max_results: Maximum number of results to return (default: 5)
        context_mode: How to return context:
            - chunk_only: Just the matching chunk
            - enhanced: Chunk with note metadata
            - full_note: Entire note content

    Returns:
        List of search results with relevance scores

    Raises:
        ValueError: If collection is not available
    """
    global available_collections

    # Check if collection is available
    if collection_name not in available_collections:
        raise ValueError(
            f"Collection '{collection_name}' is not available. "
            f"Use list_knowledge_bases() to see available collections."
        )

    coll_info = available_collections[collection_name]
    provider = coll_info["provider"]
    collection = coll_info["collection"]
    metadata = coll_info["metadata"]

    # Generate query embedding using the same provider as collection
    query_embedding = provider.generate_embedding(query)

    # Validate dimension (sanity check)
    expected_dim = metadata.get("embedding_dimension")
    if expected_dim and len(query_embedding) != expected_dim:
        raise ValueError(
            f"Embedding dimension mismatch! "
            f"Query embedding: {len(query_embedding)}, "
            f"Collection expects: {expected_dim}"
        )

    # Query ChromaDB
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=max_results,
        include=["documents", "metadatas", "distances"]
    )

    # Format results
    formatted_results = []
    for i in range(len(results["ids"][0])):
        chunk_id = results["ids"][0][i]
        chunk_text = results["documents"][0][i]
        chunk_metadata = results["metadatas"][0][i]
        distance = results["distances"][0][i]

        # Calculate similarity score (1 - cosine distance)
        similarity = 1 - distance

        result_item = {
            "chunk_id": chunk_id,
            "similarity": similarity,
            "content": chunk_text,
            "metadata": chunk_metadata
        }

        # Add context based on mode
        if context_mode == "enhanced":
            result_item["note_title"] = chunk_metadata.get("title", "Unknown")
            result_item["note_id"] = chunk_metadata.get("note_id", "Unknown")
        elif context_mode == "full_note":
            # TODO: Fetch full note content if needed
            result_item["note_title"] = chunk_metadata.get("title", "Unknown")
            result_item["full_note_available"] = False  # Placeholder

        formatted_results.append(result_item)

    return formatted_results


if __name__ == "__main__":
    # Initialize server (discover collections, test providers)
    initialize_server()

    # Start MCP server
    mcp.run()
```

**MCP Server Config (`markdown-notes-mcp-server/config.json`):**

```json
{
  "chromadb_path": "../chromadb_data"
}
```

---

## Secret Coupling and Environment Variables

### The Coupling Problem

**Identified limitation:** Pipeline and MCP server must use **same environment variable names**.

**Example:**

```bash
# Pipeline run
export OPENAI_API_KEY="sk-proj-..."
python full_pipeline.py --config work-notes-openai.json

# Stores in metadata: "embedding_api_key_ref": "${OPENAI_API_KEY}"
```

```bash
# MCP server startup (later, different shell session)
export OPENAI_API_KEY="sk-proj-..."  # MUST be same variable name!
python mcp_server.py

# Resolves "${OPENAI_API_KEY}" from environment
```

**This is a naming contract:** If pipeline uses `${OPENAI_API_KEY}`, MCP must have that exact variable.

### Why This Coupling Exists

**Alternative approaches considered:**

1. **Store API keys in ChromaDB** ‚Üí Security risk, keys in database
2. **Separate MCP config with keys** ‚Üí Duplication, sync problems
3. **Key lookup service** ‚Üí Over-engineering for simple use case
4. **Environment variable template** ‚Üí Current approach (simplest)

**Current approach is simplest and most secure:**

- ‚úÖ No secrets in files or database
- ‚úÖ Standard environment variable pattern
- ‚úÖ Easy to understand and document
- ‚ö†Ô∏è Requires naming discipline

### Best Practices for Environment Variables

**Recommended naming convention:**

```bash
# Cloud providers - use standard names
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="AIza..."
export AZURE_OPENAI_API_KEY="..."

# Custom providers
export CUSTOM_AI_API_KEY="..."
```

**Documentation for users:**

````markdown
## Environment Variables

The following environment variables must be set for both pipeline and MCP server:

- `OPENAI_API_KEY`: For OpenAI embeddings (text-embedding-3-\*)
- `GEMINI_API_KEY`: For Google Gemini embeddings
- `ANTHROPIC_API_KEY`: For Anthropic Claude (if used for LLM validation)

Set these in your shell profile (~/.bashrc, ~/.zshrc) for persistence:

```bash
export OPENAI_API_KEY="your-key-here"
```
````

Verify they're set:

```bash
echo $OPENAI_API_KEY
```

````

---

## Security Considerations

### API Key Management - DEFINITIVE APPROACH

**API keys are NEVER stored in files or databases.**

#### How It Works

**1. Pipeline Configuration (Template References):**
```json
{
  "ai_provider": {
    "embedding_model": {
      "api_key": "${OPENAI_API_KEY}"  // Template reference
    }
  }
}
````

**2. ChromaDB Metadata (Template Stored):**

```python
metadata = {
    "embedding_api_key_ref": "${OPENAI_API_KEY}",  // Template stored
    # NOT the actual key!
}
```

**3. Environment Variables (Actual Secrets):**

```bash
export OPENAI_API_KEY="sk-proj-actual-secret-key"
```

**4. Runtime Resolution:**

```python
# AIProviderConfig._resolve_api_key()
api_key_ref = "${OPENAI_API_KEY}"
env_var = api_key_ref[2:-1]  # Extract "OPENAI_API_KEY"
actual_key = os.environ.get(env_var)  # Get actual secret
```

#### What's Safe to Commit

**‚úÖ Safe to commit:**

- Pipeline config files (contain templates like `"${OPENAI_API_KEY}"`)
- MCP server config file (no secrets)
- Code and scripts

**‚ùå Never commit:**

- Actual API keys
- `.env` files with real keys
- Shell history with `export` commands

#### Git Repository Structure

```gitignore
# .gitignore - MINIMAL APPROACH

# Only ignore local overrides if needed
config.local.json
*.secret.json

# Config files with templates are SAFE to commit
# They contain "${VAR_NAME}" not actual keys
```

---

## Provider-Specific Considerations

### Embedding Dimension Compatibility

**CRITICAL: Different models have different embedding dimensions!**

| Provider | Model                  | Dimensions |
| -------- | ---------------------- | ---------- |
| Ollama   | mxbai-embed-large      | 1024       |
| OpenAI   | text-embedding-3-small | 1536       |
| OpenAI   | text-embedding-3-large | 3072       |
| Google   | text-embedding-004     | 768        |

**Each collection stores its embedding dimension in metadata:**

```python
# Stored automatically during pipeline run
metadata = {
    "embedding_model": "text-embedding-3-small",
    "embedding_dimension": 1536,  # Auto-detected
    ...
}
```

**MCP server validates at query time:**

```python
# In search_knowledge_base()
expected_dim = metadata["embedding_dimension"]
query_dim = len(query_embedding)

if query_dim != expected_dim:
    raise ValueError(f"Dimension mismatch: {query_dim} != {expected_dim}")
```

**This means:**

- ‚úÖ Each collection can use different embedding models
- ‚úÖ MCP server automatically uses correct model per collection
- ‚úÖ Cross-model queries are blocked with clear errors
- ‚ö†Ô∏è Re-indexing required if changing models

### LiteLLM Model Naming Conventions

**Model names come directly from config - LiteLLM auto-detects provider:**

```json
// Ollama models (local) - auto-detected by version format
"model": "mxbai-embed-large:latest"
"model": "llama3.1:8b"

// OpenAI models - auto-detected by name pattern
"model": "text-embedding-3-small"
"model": "gpt-4o-mini"

// Gemini models - may need explicit prefix
"model": "text-embedding-004"
// OR
"model": "gemini/text-embedding-004"

// The `provider_type` in config is for metadata/logging only
// LiteLLM uses model string format for actual routing
```

**No transformations needed** - pass model name directly from config to LiteLLM.

### Cost Considerations

**Local (Ollama):**

- ‚úÖ Free
- ‚úÖ Unlimited usage
- ‚ö†Ô∏è Requires local GPU/CPU
- ‚ö†Ô∏è Slower inference

**OpenAI:**

- üí∞ `text-embedding-3-small`: $0.02 per 1M tokens (~500 chars/token)
- üí∞ `text-embedding-3-large`: $0.13 per 1M tokens
- ‚úÖ Fast inference (~15s for 1000 chunks)
- ‚ö†Ô∏è Requires internet
- ‚ö†Ô∏è API rate limits

**Google Gemini:**

- üí∞ `text-embedding-004`: Free tier available, then $0.025 per 1M chars
- ‚úÖ Fast inference
- ‚ö†Ô∏è Requires internet

**Recommendation:**

- Development: Use Ollama (free, offline)
- Production: Consider cloud providers if speed matters
- Multi-environment: Can have different collections using different providers in same ChromaDB

---

## Testing Strategy

**Tests will be implemented AFTER the core functionality is working.**

### Testing Priority (Post-Implementation)

#### Phase 1: Core Functionality (Required)

1. ‚úÖ Test `ai_provider.py` with Ollama (local, no API key needed)
2. ‚úÖ Test metadata flow: config ‚Üí ChromaDB ‚Üí MCP reconstruction
3. ‚úÖ Test MCP collection discovery with mixed availability
4. ‚úÖ Test embedding dimension validation (mismatch detection)
5. ‚úÖ Test provider initialization failures (missing env vars)
6. ‚úÖ Test assertion errors for uninitialized provider

#### Phase 2: Cloud Providers (Optional - if API keys available)

7. ‚≠ï Test with OpenAI (requires `OPENAI_API_KEY`)
8. ‚≠ï Test with Gemini (requires `GEMINI_API_KEY`)
9. ‚≠ï Test environment variable resolution
10. ‚≠ï Test missing environment variable errors at MCP startup

#### Phase 3: Integration (Final)

11. ‚úÖ Full pipeline test: JSON ‚Üí ChromaDB with multiple collections
12. ‚úÖ MCP server startup with mixed provider types
13. ‚úÖ MCP query test with automatic provider selection
14. ‚≠ï Performance benchmarking (Ollama vs cloud providers)

### Example Test Scenarios

```python
# test_mcp_collection_discovery.py

def test_mcp_discovers_available_collections():
    """Test MCP server correctly identifies available collections"""
    # Setup: Create 3 collections
    # - collection1: Ollama (should be available)
    # - collection2: OpenAI with valid key (should be available)
    # - collection3: OpenAI without key (should be unavailable)

    # Start MCP server
    # Assert: 2 collections available, 1 warning logged
    # Assert: MCP server starts successfully
    pass


def test_mcp_exits_if_no_collections_available():
    """Test MCP server exits if zero collections available"""
    # Setup: Create collections requiring API keys
    # Don't set any environment variables

    # Try to start MCP server
    # Assert: Exits with error code
    # Assert: Error message explains no collections available
    pass


def test_collection_without_metadata_unavailable():
    """Test old collections without AI metadata are unavailable"""
    # Setup: Create collection without embedding_provider metadata

    # Start MCP server
    # Assert: Collection marked unavailable
    # Assert: Warning logged about missing metadata
    pass
```

---

## Migration Strategy

### Step-by-Step Implementation

#### Step 1: Add Dependencies

```bash
cd markdown-notes-cag-data-creator
source ../.venv/bin/activate
pip install litellm
```

#### Step 2: Create Example Configuration Files

**Pipeline config example (`configs/example-ollama.json`):**

```bash
mkdir -p configs

cat > configs/example-ollama.json << 'EOF'
{
  "collection_name": "example_notes",
  "description": "Example collection using Ollama (local)",
  "forceRecreate": false,
  "skipAiValidation": false,
  "chromadb_path": "../chromadb_data",
  "json_file": "../test-data/example.json",

  "ai_provider": {
    "provider_type": "ollama",

    "embedding_model": {
      "model": "mxbai-embed-large:latest",
      "base_url": "http://localhost:11434",
      "api_key": null
    },

    "llm_model": {
      "model": "llama3.1:8b",
      "base_url": "http://localhost:11434",
      "api_key": null
    }
  }
}
EOF
```

**Pipeline config example (`configs/example-openai.json`):**

```bash
cat > configs/example-openai.json << 'EOF'
{
  "collection_name": "example_notes_openai",
  "description": "Example collection using OpenAI",
  "forceRecreate": false,
  "skipAiValidation": false,
  "chromadb_path": "../chromadb_data",
  "json_file": "../test-data/example.json",

  "ai_provider": {
    "provider_type": "openai",

    "embedding_model": {
      "model": "text-embedding-3-small",
      "api_key": "${OPENAI_API_KEY}"
    },

    "llm_model": {
      "model": "gpt-4o-mini",
      "api_key": "${OPENAI_API_KEY}"
    }
  }
}
EOF
```

**MCP server config (`markdown-notes-mcp-server/config.json`):**

```bash
cat > markdown-notes-mcp-server/config.json << 'EOF'
{
  "chromadb_path": "../chromadb_data"
}
EOF
```

#### Step 3: Implement `ai_provider.py`

- Copy implementation from Phase 1 above
- Test basic functionality with Ollama

#### Step 4: Refactor `embedding.py`

- Implement as specified in Phase 2.1
- Test with simple embedding generation

#### Step 5: Update `storage.py`

- Modify `get_or_create_collection()` as specified in Phase 2.2
- Test metadata storage

#### Step 6: Update `full_pipeline.py`

- Implement as specified in Phase 2.3
- Test complete pipeline with Ollama

#### Step 7: Implement MCP Server

- Implement as specified in Phase 2.4
- Test collection discovery
- Test query execution

#### Step 8: Testing (Post-Implementation)

- Run all test scenarios from Testing Strategy
- Verify with multiple collections
- Test mixed provider availability

---

## Usage Examples

### Creating Collections with Different Providers

**Example 1: Create collection with Ollama (local, free):**

```bash
# No API keys needed for Ollama
cd markdown-notes-cag-data-creator

python full_pipeline.py --config configs/personal-notes-ollama.json --verbose
```

**Example 2: Create collection with OpenAI:**

```bash
# Set API key first
export OPENAI_API_KEY="sk-proj-..."

cd markdown-notes-cag-data-creator
python full_pipeline.py --config configs/work-notes-openai.json --verbose
```

**Example 3: Create multiple collections with different providers:**

```bash
# Set up all API keys
export OPENAI_API_KEY="sk-..."
export GEMINI_API_KEY="AIza..."

# Create different collections
python full_pipeline.py --config configs/work-openai.json
python full_pipeline.py --config configs/research-gemini.json
python full_pipeline.py --config configs/personal-ollama.json

# Result: 3 collections in same ChromaDB, each using different AI provider
```

### Starting MCP Server

**Example: Start with multiple collections:**

```bash
# Set API keys for collections that need them
export OPENAI_API_KEY="sk-..."
export GEMINI_API_KEY="AIza..."

# Start MCP server
cd markdown-notes-mcp-server
python mcp_server.py

# Output:
# üì¶ Collection: work_notes
#    Provider: openai
#    Model: text-embedding-3-small
#    Testing provider availability... ‚úÖ AVAILABLE
#
# üì¶ Collection: research_notes
#    Provider: gemini
#    Model: text-embedding-004
#    Testing provider availability... ‚úÖ AVAILABLE
#
# üì¶ Collection: personal_notes
#    Provider: ollama
#    Model: mxbai-embed-large:latest
#    Testing provider availability... ‚úÖ AVAILABLE
#
# üöÄ MCP Server ready with 3 available collection(s)
```

**Example: Start with missing API key:**

```bash
# Don't set GEMINI_API_KEY
export OPENAI_API_KEY="sk-..."

cd markdown-notes-mcp-server
python mcp_server.py

# Output:
# üì¶ Collection: work_notes
#    Testing provider availability... ‚úÖ AVAILABLE
#
# üì¶ Collection: research_notes
#    Testing provider availability... ‚ùå UNAVAILABLE
#    Reason: Environment variable 'GEMINI_API_KEY' not set
#
# üì¶ Collection: personal_notes
#    Testing provider availability... ‚úÖ AVAILABLE
#
# ‚ö†Ô∏è  Warning: 1 collection unavailable (research_notes)
# üöÄ MCP Server ready with 2 available collection(s)
```

---

## Summary and Recommendations

### Implementation Status: READY TO PROCEED

**All major design decisions have been finalized.**

‚úÖ **Finalized:**

1. Per-run pipeline configuration (one config per collection creation)
2. Minimal MCP server configuration (just chromadb_path)
3. ChromaDB metadata as source of truth for AI provider settings
4. MCP dynamic collection discovery at startup
5. Actual embedding generation tests for provider availability
6. Only available collections exposed to AI agents
7. Environment variable-only API key management
8. API key template references stored in metadata
9. Hard error on embedding dimension mismatch
10. Assertion-based provider initialization checks

### Key Architectural Insights

**1. Configuration Simplicity:**

- Pipeline: One config file per collection (full control, flexible)
- MCP: Minimal config (discovers everything from metadata)

**2. Metadata as Source of Truth:**

- ChromaDB stores complete AI provider configuration
- MCP reconstructs providers from metadata at startup
- No configuration duplication

**3. Dynamic Availability:**

- Collections can become available/unavailable based on environment
- MCP adapts automatically to what's available
- Graceful degradation (warnings, not errors)

**4. Security by Design:**

- No secrets in files or database
- Only template references stored
- Standard environment variable pattern

**5. Multi-Provider Support:**

- Multiple collections can use different providers
- Same ChromaDB, different AI backends
- Automatic provider selection per query

### Secret Coupling - Accepted Trade-off

**The limitation:**

- Pipeline and MCP must use same environment variable names
- Example: Both must have `OPENAI_API_KEY` (not different names)

**Why it's acceptable:**

- Simplest approach (vs. complex alternatives)
- Standard environment variable pattern
- Easy to document and understand
- Common practice in cloud/container deployments

### Recommended Implementation Order

1. **Week 1: Foundation**

   - Implement `ai_provider.py`
   - Create example config files
   - Add LiteLLM dependency
   - Test basic functionality

2. **Week 2: Pipeline Integration**

   - Refactor `embedding.py`
   - Update `storage.py`
   - Update `full_pipeline.py`
   - Test metadata flow

3. **Week 3: MCP Server**

   - Implement collection discovery
   - Implement dynamic provider loading
   - Add query validation
   - Test with Claude Desktop

4. **Week 4: Testing & Polish**
   - Comprehensive testing
   - Multi-provider scenarios
   - Update documentation
   - Performance benchmarking

### Next Actions

1. **Install LiteLLM:**

   ```bash
   pip install litellm
   ```

2. **Create `ai_provider.py`:**

   - Copy implementation from Phase 1
   - Test with Ollama

3. **Create example config files:**

   - Ollama example
   - OpenAI example
   - Gemini example

4. **Begin refactoring:**
   - Start with `embedding.py`
   - Then `storage.py`
   - Then `full_pipeline.py`
   - Finally MCP server

---

## Conclusion

**The AI provider abstraction layer is implementation-ready with a clear, elegant architecture.**

**Key Benefits:**

1. ‚úÖ Flexibility: Multiple providers, per-collection choice
2. ‚úÖ Security: No secrets in files or database, only env vars
3. ‚úÖ Simplicity: Minimal config, metadata-driven discovery
4. ‚úÖ Robustness: Dynamic availability checking, graceful degradation
5. ‚úÖ Maintainability: Clean separation, source of truth in metadata

**Known Limitations:**

- ‚ö†Ô∏è Secret coupling: Environment variable naming must match
- ‚ö†Ô∏è Backward compatibility: Old collections without metadata unavailable
- ‚ö†Ô∏è Re-indexing required: Changing embedding model requires re-creation

**Recommendation: Proceed with implementation following the plan above.**

The architecture balances simplicity, security, and flexibility while providing a clear upgrade path and robust error handling.
